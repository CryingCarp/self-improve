{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# Optional: Configure tracing to visualize and debug the agent\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Tool Use\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", base_url=\"https://api.chsdw.top/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "from langchain_community.utilities import BingSearchAPIWrapper, BraveSearchWrapper, WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun\n",
    "import requests\n",
    "\n",
    "import numexpr as ne \n",
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.contentsafety.models import TextCategory, AnalyzeTextOptions\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "import diskcache as dc\n",
    "\n",
    "\n",
    "class GoogleSearchTool(BaseTool):\n",
    "    cache = Field(init=True)\n",
    "    google = Field(init=True)\n",
    "    def __init__(self, name: str = \"google_search\", description: str = \"Search Google for recent results. Input should be a search query. \", cache_dir: str = \".cache/google_knowledge_graph_tool\"):\n",
    "        super().__init__(name=name, description=description, cache_dir=cache_dir)\n",
    "        self.cache = dc.Cache(cache_dir)\n",
    "        self.google = GoogleSearchAPIWrapper()\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        if query in self.cache:\n",
    "            print(\"Cache hit for text:\", query)\n",
    "            return self.cache[query]\n",
    "        else:\n",
    "            result = self.google.results(query=query, num_results=1)\n",
    "            self.cache[query] = result\n",
    "        return result\n",
    "\n",
    "class BingSearchTool(BaseTool):\n",
    "    cache = Field(init=True)\n",
    "    bing = Field(init=True)\n",
    "    def __init__(self, name: str = \"bing_search\", description: str = \"Search Bing for recent results. Input should be a search query. \",cache_dir: str = \".cache/bing_search_tool\"):\n",
    "        super().__init__(name=name, description=description, cache_dir=cache_dir)\n",
    "        self.cache = dc.Cache(cache_dir)\n",
    "        self.bing = BingSearchAPIWrapper()\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        if query in self.cache:\n",
    "            print(\"Cache hit for text:\", query)\n",
    "            return self.cache[query]\n",
    "        else:\n",
    "            result = self.bing.results(query=query, num_results=1)\n",
    "            self.cache[query] = result\n",
    "        return result\n",
    "    \n",
    "class BraveSearchTool(BaseTool):\n",
    "    cache = Field(init=True)\n",
    "    brave = Field(init=True)\n",
    "    def __init__(self, name: str = \"brave_search\", \n",
    "                 description: str = \"A search engine. useful for when you need to answer questions about current events. Input should be a search query. \", \n",
    "                 cache_dir: str = \".cache/brave_search_tool\", api_key: str = None, search_kwargs: dict = {\"count\": 1}):\n",
    "        super().__init__(name=name, description=description, cache_dir=cache_dir)\n",
    "        self.cache = dc.Cache(cache_dir)\n",
    "        self.brave = BraveSearchWrapper(api_key=api_key, search_kwargs=search_kwargs or {})\n",
    "    \n",
    "    def _run(self, query: str) -> str:\n",
    "        if query in self.cache:\n",
    "            print(\"Cache hit for text:\", query)\n",
    "            return self.cache[query]\n",
    "        else:\n",
    "            result = self.brave.run(query=query)\n",
    "            self.cache[query] = result\n",
    "        return result\n",
    "\n",
    "class WikidataTool(WikidataQueryRun):\n",
    "    name = \"wikidata\"\n",
    "\n",
    "    cache = Field(init=True)\n",
    "\n",
    "    def __init__(self, api_wrapper: WikidataAPIWrapper = WikidataAPIWrapper(), cache_dir: str = \".cache/wikidata_tool\"):\n",
    "        super().__init__(api_wrapper=api_wrapper)\n",
    "        self.cache = dc.Cache(cache_dir)\n",
    "\n",
    "    def _run(self, text: str) -> str:\n",
    "        if text in self.cache:\n",
    "            print(\"Cache hit for text:\", text)\n",
    "            return self.cache[text]\n",
    "        \n",
    "        results = super()._run(text)\n",
    "\n",
    "        self.cache[text] = results\n",
    "        \n",
    "        return results\n",
    "\n",
    "class WikipediaTool(WikipediaQueryRun):\n",
    "    name = \"wikipedia\"\n",
    "\n",
    "    cache = Field(init=True)\n",
    "\n",
    "    def __init__(self, api_wrapper: WikipediaAPIWrapper = WikipediaAPIWrapper(top_k_results=1), cache_dir: str = \".cache/wikipedia_tool\"):\n",
    "        super().__init__(api_wrapper=api_wrapper)\n",
    "        self.cache = dc.Cache(cache_dir)\n",
    "\n",
    "    def _run(self, text: str) -> str:\n",
    "        if text in self.cache:\n",
    "            print(\"Cache hit for text:\", text)\n",
    "            return self.cache[text]\n",
    "        \n",
    "        results = super()._run(text)\n",
    "\n",
    "        self.cache[text] = results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "class GoogleKnowledgeGraphTool(BaseTool):\n",
    "    name = \"google_knowledge_graph\"\n",
    "    description = (\n",
    "        \"This tool searches for entities in the Google Knowledge Graph. \"\n",
    "        \"It provides information about people, places, things, and concepts. \"\n",
    "        \"Input should be an entity name.\"\n",
    "    )\n",
    "    api_key: str = Field(..., description=\"Google Knowledge Graph Search API key\")\n",
    "    cache = Field(init=True)\n",
    "\n",
    "    def __init__(self, api_key: str, cache_dir: str = \".cache/google_knowledge_graph_tool\"):\n",
    "        super().__init__(api_key=api_key)\n",
    "        self.cache = dc.Cache(cache_dir)\n",
    "\n",
    "    def _run(self, query: str, limit: int = 1) -> str:\n",
    "        if query in self.cache:\n",
    "            print(\"Cache hit for text:\", query)\n",
    "            return self.cache[query]\n",
    "\n",
    "        service_url = \"https://kgsearch.googleapis.com/v1/entities:search\"\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"limit\": limit,\n",
    "            \"indent\": True,\n",
    "            \"key\": self.api_key,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(service_url, params=params)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Failed to retrieve data: {str(e)}\"\n",
    "\n",
    "        data = response.json()\n",
    "        self.cache[query] = data\n",
    "        return data\n",
    "\n",
    "    def _format_results(self, data: dict) -> str:\n",
    "        results = data.get(\"itemListElement\", [])\n",
    "        formatted_results = []\n",
    "\n",
    "        for element in results:\n",
    "            result = element.get(\"result\", {})\n",
    "            name = result.get(\"name\", \"N/A\")\n",
    "            score = element.get(\"resultScore\", 0)\n",
    "            formatted_results.append(f\"{name} ({score})\")\n",
    "\n",
    "        return \"\\n\".join(formatted_results) if formatted_results else \"No results found.\"\n",
    "\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"calculator\"\n",
    "    description = (\"Useful when you need to calculate the value of a mathematical expression, including basic arithmetic operations. \"\n",
    "                   \"Use this tool for math operations. \"\n",
    "                   \"Input should strictly follow the numuxpr syntax. \")\n",
    "\n",
    "    def _run(self, expression: str):\n",
    "      try:\n",
    "        result = ne.evaluate(expression).item()\n",
    "        return f\"The result of the expression of <{expression}> is: {result}.\"\n",
    "      except Exception as e:\n",
    "        # return \"This is not a numexpr valid syntax. Try a different syntax.\"\n",
    "        return f\"Error in calculation: {str(e)}\"\n",
    "      \n",
    "class PythonREPLTool(BaseTool):\n",
    "    name = \"python_repl\"\n",
    "    description = (\"A Python shell. Use this to execute python commands. Input should be a valid python command. \"\n",
    "                   \"If you want to see the output of a value, you should print it out with `print(...)`.\")\n",
    "    def _run(self, code: str) -> str:\n",
    "        try:\n",
    "            result = PythonREPL().run(code)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "class PerspectiveTool(BaseTool):\n",
    "    name = \"perspective\"\n",
    "    description = (\"This tool analyzes text for safety using Google Perspective API\"\n",
    "                   \"It detects categories such as hate, self-harm, sexual content, and violence.\"\n",
    "                   )\n",
    "    # api_key: str = Field(default=os.environ.get(\"PERSPECTIVE_API_KEY\"), description=\"Google Perspective API key\")\n",
    "    client = Field(init=True)\n",
    "    cache = Field(init=True)\n",
    "\n",
    "    def __init__(self, api_key: str=os.environ.get(\"GOOGLE_PERSPECTIVE_API_KEY\"), cache_dir: str = \".cache/perspective_tool\"):\n",
    "        super().__init__()\n",
    "        self.client = discovery.build(\"commentanalyzer\", \"v1alpha1\", \n",
    "                                      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "                                      developerKey=api_key,\n",
    "                                      static_discovery=False,)\n",
    "        self.cache = dc.Cache(cache_dir)\n",
    "        \n",
    "    def _run(self, text: str):\n",
    "        if text in self.cache:\n",
    "            print(\"Cache hit for text:\", text)\n",
    "            return self.cache[text]\n",
    "\n",
    "        PERSPECTIVE_API_ATTRIBUTES = (\n",
    "            'TOXICITY',\n",
    "            'SEVERE_TOXICITY',\n",
    "            'IDENTITY_ATTACK',\n",
    "            'INSULT',\n",
    "            'THREAT',\n",
    "            'PROFANITY',\n",
    "            'SEXUALLY_EXPLICIT',\n",
    "            'FLIRTATION'\n",
    "        )\n",
    "\n",
    "        analyze_request = {\n",
    "            'comment': {'text': text},\n",
    "            'requestedAttributes': {attr: {} for attr in PERSPECTIVE_API_ATTRIBUTES}\n",
    "        }\n",
    "\n",
    "        response = self.client.comments().analyze(body=analyze_request).execute()\n",
    "        # Cache the result\n",
    "        self.cache[text] = response\n",
    "        return response\n",
    "\n",
    "class AzureContentModerationTool(BaseTool):\n",
    "    name = \"azure_content_moderation\"\n",
    "    description = (\n",
    "        \"This tool analyzes text for safety using Azure AI Content Safety. \"\n",
    "        \"It detects categories such as hate, self-harm, sexual content, and violence. \"\n",
    "        \"Input should be a text string. \"\n",
    "    )\n",
    "\n",
    "    key: str = Field(default=os.environ.get(\"AZURE_CONTENT_SAFETY_KEY\"), description=\"Azure Content Safety API key\")\n",
    "    endpoint: str = Field(default=os.environ.get(\"AZURE_CONTENT_SAFETY_ENDPOINT\"), description=\"Azure Content Safety endpoint\")\n",
    "    client: ContentSafetyClient = Field(init=True)\n",
    "    cache = Field(init=True)\n",
    "\n",
    "    def __init__(self, endpoint: str = None, key: str = None, cache_dir: str = \".cache/azure_content_moderation_tool\"):\n",
    "        super().__init__(key=key, endpoint=endpoint)\n",
    "        self.client = ContentSafetyClient(self.endpoint, AzureKeyCredential(self.key))\n",
    "        self.cache = dc.Cache(cache_dir)\n",
    "\n",
    "    def _run(self, text: str) -> str:\n",
    "        if text in self.cache:\n",
    "            print(\"Cache hit for text:\", text)\n",
    "            return self.cache[text]\n",
    "        \n",
    "        request = AnalyzeTextOptions(text=text)\n",
    "\n",
    "        try:\n",
    "            response = self.client.analyze_text(request)\n",
    "        except HttpResponseError as e:\n",
    "            error_message = \"Analyze text failed.\"\n",
    "            if e.error:\n",
    "                error_message += f\" Error code: {e.error.code}. Error message: {e.error.message}\"\n",
    "            return error_message\n",
    "\n",
    "        results = {\n",
    "            \"hate\": self._get_severity(response, TextCategory.HATE),\n",
    "            \"self_harm\": self._get_severity(response, TextCategory.SELF_HARM),\n",
    "            \"sexual\": self._get_severity(response, TextCategory.SEXUAL),\n",
    "            \"violence\": self._get_severity(response, TextCategory.VIOLENCE),\n",
    "        }\n",
    "\n",
    "        formatted_results = self._format_results(results)\n",
    "\n",
    "        self.cache[text] = formatted_results\n",
    "\n",
    "        return formatted_results\n",
    "\n",
    "    def _get_severity(self, response, category: TextCategory):\n",
    "        result = next((item for item in response.categories_analysis if item.category == category), None)\n",
    "        return result.severity if result else \"Not found\"\n",
    "\n",
    "    def _format_results(self, results: dict) -> str:\n",
    "        return \"\\n\".join(f\"{category.capitalize()} severity: {severity} outof 7.\" for category, severity in results.items())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit for text: I hate you\n",
      "{'attributeScores': {'INSULT': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.4378843, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.4378843, 'type': 'PROBABILITY'}}, 'TOXICITY': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.6827122, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.6827122, 'type': 'PROBABILITY'}}, 'THREAT': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.043955702, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.043955702, 'type': 'PROBABILITY'}}, 'IDENTITY_ATTACK': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.28883415, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.28883415, 'type': 'PROBABILITY'}}, 'SEVERE_TOXICITY': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.0394905, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.0394905, 'type': 'PROBABILITY'}}, 'SEXUALLY_EXPLICIT': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.01480327, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.01480327, 'type': 'PROBABILITY'}}, 'FLIRTATION': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.18296637, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.18296637, 'type': 'PROBABILITY'}}, 'PROFANITY': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.3351773, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.3351773, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n"
     ]
    }
   ],
   "source": [
    "google_search = GoogleSearchTool()\n",
    "# print(google_search.run(\"What is the capital of France?\"))\n",
    "bing_search = BingSearchTool()\n",
    "# print(bing_search.run(\"What is the capital of China?\"))\n",
    "brave = BraveSearchTool(api_key=os.environ.get(\"BRAVE_API_KEY\"))\n",
    "# print(brave.run(\"What is the capital of India?\"))\n",
    "azure_content_moderation = AzureContentModerationTool(endpoint=os.environ.get(\"AZURE_CONTENT_SAFETY_ENDPOINT\"), key=os.environ.get(\"AZURE_CONTENT_SAFETY_KEY\"))\n",
    "# print(azure_content_moderation.run(\"I hate you\"))\n",
    "wikidata = WikidataTool()\n",
    "# print(wikidata.run(\"Japan\"))\n",
    "wikipedia = WikipediaTool()\n",
    "# print(wikipedia.run(\"Japan\"))\n",
    "google_knowledge_graph = GoogleKnowledgeGraphTool(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "# google_knowledge_graph.run(\"China\")\n",
    "calculator = CalculatorTool()\n",
    "# calculator.run(\"2+2\")\n",
    "python_repl = PythonREPLTool()\n",
    "# python_repl.run(\"print('Hello, World!')\")\n",
    "perspective = PerspectiveTool()\n",
    "print(perspective.run(\"I hate you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造ToolList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tools_descriptions(tools:list):\n",
    "    tools_descriptions = []\n",
    "    for tool in tools:\n",
    "        tools_descriptions.append(f\"{tool.name} - {tool.description}\")\n",
    "    return \"\\n\\n\".join(tools_descriptions)\n",
    "\n",
    "def get_tools_dict(tools:list)->dict:\n",
    "    tools_dict = {}\n",
    "    for tool in tools:\n",
    "        tools_dict[tool.name.lower()] = tool\n",
    "    return tools_dict\n",
    "\n",
    "tools = [google_search, bing_search, brave, azure_content_moderation, wikidata, wikipedia, google_knowledge_graph, calculator, python_repl, perspective]\n",
    "\n",
    "tools_descriptions = get_tools_descriptions(tools)\n",
    "tools_dict = get_tools_dict(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Tool(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    tool: str = Field(..., description=\"The tool used to find information which are useful for the question.\")\n",
    "    input: str = Field(..., description=\"The input for the tool.\")\n",
    "    \n",
    "\n",
    "\n",
    "class ToolList(BaseModel):\n",
    "    \"\"\"Identifying all the tools needed to answer the question.\"\"\"\n",
    "\n",
    "    tool_list: List[Tool]\n",
    "\n",
    "\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=ToolList)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You have access to the following tools:\\n\\n\"\n",
    "            \"{tools_description}\\n\\n\"\n",
    "            \"--------------------------------\\n\"\n",
    "            \"Inspect each tool's description and the input requirements carefully.\"\n",
    "            \"Identify all the tools you may need to use and the corresponding tool input text based on the user's current step. \"\n",
    "            \"You should also consider the overall context of the user's question. \"\n",
    "            \"{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(tools_description=tools_descriptions ,format_instructions=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You have access to the following tools:\n",
      "\n",
      "google_search - Search Google for recent results. Input should be a search query. \n",
      "\n",
      "bing_search - Search Bing for recent results. Input should be a search query. \n",
      "\n",
      "brave_search - A search engine. useful for when you need to answer questions about current events. Input should be a search query. \n",
      "\n",
      "azure_content_moderation - This tool analyzes text for safety using Azure AI Content Safety. It detects categories such as hate, self-harm, sexual content, and violence. Input should be a text string. \n",
      "\n",
      "wikidata - A wrapper around Wikidata. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be the exact name of the item you want information about or a Wikidata QID.\n",
      "\n",
      "wikipedia - A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "\n",
      "google_knowledge_graph - This tool searches for entities in the Google Knowledge Graph. It provides information about people, places, things, and concepts. Input should be an entity name.\n",
      "\n",
      "calculator - Useful when you need to calculate the value of a mathematical expression, including basic arithmetic operations. Use this tool for math operations. Input should strictly follow the numuxpr syntax. \n",
      "\n",
      "python_repl - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n",
      "\n",
      "perspective - This tool analyzes text for safety using Google Perspective APIIt detects categories such as hate, self-harm, sexual content, and violence.\n",
      "\n",
      "--------------------------------\n",
      "Identify all the tools you may need to use and the corresponding tool inputs text based on the user's questions.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Identifying all the tools needed to answer the question.\", \"properties\": {\"tool_list\": {\"title\": \"Tool List\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Tool\"}}}, \"required\": [\"tool_list\"], \"definitions\": {\"Tool\": {\"title\": \"Tool\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"tool\": {\"title\": \"Tool\", \"description\": \"The tool used to find information which are useful for the question.\", \"type\": \"string\"}, \"input\": {\"title\": \"Input\", \"description\": \"The input for the tool.\", \"type\": \"string\"}}, \"required\": [\"tool\", \"input\"]}}}\n",
      "```\n",
      "Human: If a bag of marbles costs $20 and the price increases by 20% of the original price every two months, how much would a bag of marbles cost after 36 months?\n"
     ]
    }
   ],
   "source": [
    "query = \"If a bag of marbles costs $20 and the price increases by 20% of the original price every two months, how much would a bag of marbles cost after 36 months?\"\n",
    "\n",
    "print(prompt.invoke(query).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_dict['google knowledge graph'].run('Google Knowledge Graph')\n",
    "tools_dict['wikipedia'].run('How to make steak recipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, TypedDict\n",
    "\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser, JsonOutputParser\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Plan)\n",
    "\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"For the given objective, come up with a simple step by step plan. \"\n",
    "            \"This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \"\n",
    "            \"The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps. \\n\"\n",
    "        ),\n",
    "        (\"human\", \"Here is the question you need to plan. \\n{messages}\\n{format_instructions}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = planner_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature=0, base_url=\"https://api.chsdw.top/v1\"\n",
    ") | parser\n",
    "response = planner.invoke(input={\"messages\": \"Which sports event was first held at Worcester, Massachusetts in 1927?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "critic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
