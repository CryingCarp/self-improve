{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T10:07:35.756112Z",
     "start_time": "2025-01-22T10:07:33.895847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"self-correct\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://127.0.0.1:7890\""
   ],
   "id": "efeb283497cde6c9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T10:07:39.740288Z",
     "start_time": "2025-01-22T10:07:38.166836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "meta_info = {\n",
    "\t\"dataset_name\": 'hotpot_qa',\n",
    "\t\"mode\": \"self-improve\",\n",
    "\t\"base_mode\": \"cot\",\n",
    "\t\"model\": \"gpt-3.5-turbo\",\n",
    "\t\"num_samples\": 1000,\n",
    "\t\"top_p\": 0.95,\n",
    "\t\"temperature\": 0,\n",
    "\t\"seed\": 42,\n",
    "\t\"batch_size\": 100\n",
    "}\n",
    "assert meta_info[\"mode\"] == \"self-improve\"\n",
    "assert meta_info[\"dataset_name\"] in [\"hotpot_qa\", \"trivia_qa\", \"ambig_qa\"], \"Invalid dataset name\"\n",
    "\n",
    "ROOT_DIR = \"D:\\Projects\\self-improve\"\n",
    "processed_data_path = os.path.join(ROOT_DIR, \"output\", \"inference\", meta_info[\"model\"], meta_info[\"dataset_name\"], meta_info[\"mode\"], f\"with_question_before_fusion_{meta_info['base_mode']}_num_samples_{meta_info['num_samples']}_top_p_{meta_info['top_p']}_temperature_{meta_info['temperature']}_seed_{meta_info['seed']}.jsonl\")\n",
    "print(\"Loading processed data from:\", processed_data_path)\n",
    "dataset = []\n",
    "try:\n",
    "    with open(processed_data_path, 'r', encoding='utf-8') as file:\n",
    "        # Attempt to load JSON data\n",
    "        for line in file:\n",
    "\t        dataset.append(json.loads(line))\n",
    "        print(\"Data loaded successfully from:\", processed_data_path)\n",
    "    print(\"Sample data:\", dataset[0])\n",
    "\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON from '{processed_data_path}': {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the data: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Setting the save_results_path\")\n",
    "save_results_path = os.path.join(ROOT_DIR, \"output\", \"ablation\", meta_info[\"model\"], meta_info[\"dataset_name\"], meta_info[\"mode\"], f\"without_fusion_{meta_info['base_mode']}_num_samples_{meta_info['num_samples']}_top_p_{meta_info['top_p']}_temperature_{meta_info['temperature']}_seed_{meta_info['seed']}.jsonl\")\n",
    "print(\"Results will be saved to:\", save_results_path)\n",
    "\n",
    "\n",
    "model = ChatOpenAI(\n",
    "\tmodel_name=meta_info[\"model\"],\n",
    "\ttop_p=meta_info[\"top_p\"],\n",
    "\ttemperature=meta_info[\"temperature\"],\n",
    "\tseed=meta_info[\"seed\"],\n",
    "\topenai_api_base=\"https://api.chsdw.top/v1\"\n",
    ")\n",
    "\n"
   ],
   "id": "f79dc6013e0a4dcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from: D:\\Projects\\self-improve\\output\\inference\\gpt-3.5-turbo\\hotpot_qa\\self-improve\\with_question_before_fusion_cot_num_samples_1000_top_p_0.95_temperature_0_seed_42.jsonl\n",
      "Data loaded successfully from: D:\\Projects\\self-improve\\output\\inference\\gpt-3.5-turbo\\hotpot_qa\\self-improve\\with_question_before_fusion_cot_num_samples_1000_top_p_0.95_temperature_0_seed_42.jsonl\n",
      "Sample data: {'context': '', 'question': 'Were Scott Derrickson and Ed Wood of the same nationality?', 'answer': ['yes'], 'guidance': 'Step 1: Determine the nationalities of Scott Derrickson and Ed Wood.\\n- Research the nationality of Scott Derrickson.\\n- Research the nationality of Ed Wood.\\n\\nError-prone points:\\n- Inaccurate or conflicting information about their nationalities in sources.\\n- Confusion with individuals who have similar names.\\n- Lack of clear documentation of their nationalities.', 'messages': [{'content': 'You are a reactive agent. Given a question or problem, your job is to select the appropriate tools to answer the question or solve the problem. You should consider the guidance provided by the question planner and error prone points identifier, and the tool results are reliable. If you find the answer from the tool results, you should provide the answer.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': 'd2518f15-ff1c-4a8d-9c15-46ea80ab6036'}, {'content': 'Question: Were Scott Derrickson and Ed Wood of the same nationality?Guidance: Step 1: Determine the nationalities of Scott Derrickson and Ed Wood.\\n- Research the nationality of Scott Derrickson.\\n- Research the nationality of Ed Wood.\\n\\nError-prone points:\\n- Inaccurate or conflicting information about their nationalities in sources.\\n- Confusion with individuals who have similar names.\\n- Lack of clear documentation of their nationalities.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '5d544aed-9e1c-411e-b1ba-bd1feb898802', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_VluwWPS7W91c4VZDBl7G991R', 'function': {'arguments': '{\"query\": \"Scott Derrickson\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'call_7K2XUtr92h0p7PzTewoEjN1Y', 'function': {'arguments': '{\"query\": \"Ed Wood\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 44, 'prompt_tokens': 700, 'total_tokens': 744, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run-40f78d4d-468c-4bb2-ab54-31b53048c822-0', 'example': False, 'tool_calls': [{'name': 'wikipedia', 'args': {'query': 'Scott Derrickson'}, 'id': 'call_VluwWPS7W91c4VZDBl7G991R', 'type': 'tool_call'}, {'name': 'wikipedia', 'args': {'query': 'Ed Wood'}, 'id': 'call_7K2XUtr92h0p7PzTewoEjN1Y', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 700, 'output_tokens': 44, 'total_tokens': 744}}, {'content': 'Page: Scott Derrickson\\nSummary: Scott Derrickson (born July 16, 1966) is an American filmmaker. He is best known for his work in the horror genre, directing films such as The Exorcism of Emily Rose (2005), Sinister (2012), and The Black Phone (2021). He is also known for the superhero film Doctor Strange (2016), based on the Marvel Comics character.\\n\\nPage: The Black Phone\\nSummary: The Black Phone is a 2021 American supernatural horror film directed by Scott Derrickson, and written by Derrickson and C. Robert Cargill. It stars Mason Thames as Finney, a teenage boy abducted by a serial child killer known colloquially as The Grabber (Ethan Hawke). When Finney encounters a mystical black rotary phone in captivity, he uses it to plot his escape by communicating with the ghosts of The Grabber\\'s slain victims. Madeleine McGraw, Jeremy Davies, and James Ransone also feature in the principal cast. Derrickson and Cargill produced The Black Phone in association with Blumhouse Productions CEO Jason Blum. Universal Pictures oversaw the film\\'s commercial distribution, and funding was sourced through a Universal–Blumhouse co-production pact and tax subsidies from the North Carolina state government.\\nThe idea of The Black Phone arose from Derrickson and Cargill\\'s adaptation of Joe Hill\\'s short story of the same name. Derrickson struggled to produce additional ideas that supplemented the short story, shifting his attention to other filmmaking endeavors. The film remained dormant until he resigned from Doctor Strange in the Multiverse of Madness (2022) over creative differences. Derrickson used his childhood experiences in suburban Denver, Colorado, to develop The Black Phone story. Principal photography began in February 2021 on a $16–18 million budget, and wrapped the following month. Shooting took place on sets and on location in Wilmington, North Carolina. Mark Korven composed the film\\'s score, which drew on modern and vintage synthesizer sounds.\\nThe Black Phone premiered at Fantastic Fest on September 25, 2021, and opened in US theaters after several delays on June 24, 2022. It was an unexpected hit at the box office and finished its theatrical run grossing $161.4 million globally. The film received generally positive reviews from the critics, who praised its performances but were divided on its concept. The success of The Black Phone spawned the in-canon short film \"Dreamkill\", also directed by Derrickson and written by him and Cargill, released in 2023 as part of the anthology film V/H/S/85. A sequel, The Black Phone 2, is scheduled for release on October 17, 2025.\\n\\nPage: The Gorge (film)\\nSummary: The Gorge is an upcoming American survival action film directed by Scott Derrickson and written by Zach Dean. The film stars Miles Teller, Anya Taylor-Joy, and Sigourney Weaver.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'wikipedia', 'id': '379b4648-9450-47fc-b2ab-0177643bb616', 'tool_call_id': 'call_VluwWPS7W91c4VZDBl7G991R', 'artifact': None, 'status': 'success'}, {'content': 'Page: Ed Wood\\nSummary: Edward Davis Wood Jr. (October 10, 1924 – December 10, 1978) was an American filmmaker, actor, and pulp novelist.\\nIn the 1950s, Wood directed several low-budget science fiction, crime and horror films that later became cult classics, notably Glen or Glenda (1953), Jail Bait (1954), Bride of the Monster (1955), Plan 9 from Outer Space (1957) and Night of the Ghouls (1959). In the 1960s and 1970s, he moved towards sexploitation and pornographic films such as The Sinister Urge (1960), Orgy of the Dead (1965) and Necromania (1971), and wrote over 80 lurid pulp crime and sex novels.\\nNotable for their campy aesthetics, technical errors, unsophisticated special effects, use of poorly-matched stock footage, eccentric casts, idiosyncratic stories and non sequitur dialogue, Wood\\'s films remained largely obscure until he was posthumously awarded a Golden Turkey Award for Worst Director of All Time in 1980, renewing public interest in his life and work.\\nFollowing the publication of Rudolph Grey\\'s 1992 oral biography Nightmare of Ecstasy: The Life and Art of Edward D. Wood Jr., a biopic of his life, Ed Wood (1994), was directed by Tim Burton. Starring Johnny Depp as Wood and Martin Landau as Bela Lugosi, the film received critical acclaim and various awards, including two Academy Awards.\\n\\n\\n\\nPage: Ed Wood (film)\\nSummary: Ed Wood is a 1994 American biographical comedy-drama film directed and produced by Tim Burton and starring Johnny Depp as Ed Wood, the eponymous cult filmmaker. The film concerns the period in Wood\\'s life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau. Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\\nThe film was conceived by writers Scott Alexander and Larry Karaszewski. Alexander first proposed it as a documentary, when he was a student at the USC School of Cinematic Arts. Years later, irritated at being thought of solely as writers for family films with their work on Problem Child (1990) and its 1991 sequel, Alexander and Karaszewski struck a deal with Burton and Denise Di Novi to produce Ed Wood. Initially, Michael Lehmann was chosen to direct the project, but due to scheduling conflicts with his work on the film Airheads (1994), he had to vacate the director\\'s position which was taken over by Tim Burton.\\nEd Wood was originally in development at Columbia Pictures, but the studio put the film in \"turnaround\" over Burton\\'s decision to shoot in black-and-white. Ed Wood was taken to Walt Disney Studios, which produced the film through its Touchstone Pictures banner. The film proved financially unsuccessful, returning only $13.8 million against an $18 million budget, but was met with critical acclaim upon release, with particular praise for Depp and Landau\\'s performances and the makeup, and won two Academy Awards: Best Supporting Actor for Landau and Best Makeup for Rick Baker (who designed Landau\\'s prosthetic makeup), Ve Neill and Yolanda Toussieng. The film is now considered to be a cult classic and one of Burton\\'s best works.\\n\\n\\n\\nPage: Edward Wood\\nSummary: Edward or Ed Wood may refer to:\\n\\nEdward Wood (MP) (fl. 1584–86), English politician\\nSir Edward Wood (diplomat), British envoy to Sweden in the 1670s\\nEdward Wood (British Army officer) (1841–1898)\\nEd Wood (1924–1978), American filmmaker\\nEd Wood (film), a 1994 film based on the life of the filmmaker\\nEdward Wood, 1st Earl of Halifax (1881–1959), British politician\\nEdward J. Wood (1866–1956), leader of the Church of Jesus Christ of Latter-day Saints in Alberta, Canada\\nEdward Allan Wood (1872–1930), British Army officer\\nEdward Rogers Wood (1866–1941), financier in Canadian business\\nEdward Wood (priest), archdeacon of Mashonaland, 1946–1960\\nEdward John Wood, footballer known as Jackie Wood\\nEd Wood (engineer) (born 1968), former chief designer for the Williams Formula One team\\nEd Wood (elm cultivar), a Chinese elm cultivar\\n\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'wikipedia', 'id': 'dc7c71a6-81a8-4dc0-a438-b179f3055cf2', 'tool_call_id': 'call_7K2XUtr92h0p7PzTewoEjN1Y', 'artifact': None, 'status': 'success'}, {'content': 'Scott Derrickson is an American filmmaker, and Ed Wood was also an American filmmaker. Thus, Scott Derrickson and Ed Wood were of the same nationality, both being American.', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 36, 'prompt_tokens': 2362, 'total_tokens': 2398, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run-cc1b7197-18db-404f-bda4-7f9498b4274a-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 2362, 'output_tokens': 36, 'total_tokens': 2398}}], 'fusion': 'Scott Derrickson is an American filmmaker, and Ed Wood was also an American filmmaker.', 'prediction': 'Yes'}\n",
      "Setting the save_results_path\n",
      "Results will be saved to: D:\\Projects\\self-improve\\output\\ablation\\gpt-3.5-turbo\\hotpot_qa\\self-improve\\without_fusion_cot_num_samples_1000_top_p_0.95_temperature_0_seed_42.jsonl\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from langgraph.graph import add_messages\n",
    "# from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "# from typing import Sequence\n",
    "# from typing_extensions import TypedDict, Annotated\n",
    "# from langgraph.managed.is_last_step import RemainingSteps\n",
    "#\n",
    "#\n",
    "# class State(TypedDict):\n",
    "# \tquestion: str\n",
    "# \tguidance: str\n",
    "# \tremaining_steps: RemainingSteps\n",
    "# \tmessages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "# \tinitial_answer: str\n",
    "# \tprediction: str\n",
    "#\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "#\n",
    "# guidance_prompt = ChatPromptTemplate.from_messages(\n",
    "# \t[\n",
    "# \t\t(\n",
    "# \t\t\t\"system\",\n",
    "# \t\t\t\"You are a question planner and error prone points identifier. Given a question or problem, your job is to come up with a step by step plan, and you should also identify the most error-prone points for each step, following them closely behind each step. The plan and the error prone points will then be used to guide the selection of subsequent tools and the corresponding tool inputs. The tool results should always be considered as reliable. Do not add any superfluous steps. Make sure that each step has all the information needed - do not skip steps. You should focus on the logic of how to solve the problem, rather than actually solving it.\"\n",
    "# \t\t),\n",
    "# \t\t(\n",
    "# \t\t\t\"user\",\n",
    "# \t\t\t\"Question: {question}\"\n",
    "# \t\t)\n",
    "# \t])\n",
    "# guidance_generator = guidance_prompt | model\n",
    "#\n",
    "#\n",
    "# async def guidance_node(state: State) -> State:\n",
    "# \tassert state[\"question\"] is not None, \"Question is required\"\n",
    "# \tquestion:str = state[\"question\"]\n",
    "# \tguidance:AIMessage = await guidance_generator.ainvoke(input={\"question\": question})\n",
    "# \tstate[\"guidance\"] = guidance.content\n",
    "# \treturn state\n",
    "#\n",
    "# from langchain_core.messages import AIMessage\n",
    "# from langchain_community.utilities.wikidata import WikidataAPIWrapper\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "# from agent.utils.tools import GoogleSearchTool, GoogleKnowledgeGraphTool, WikidataTool, WikipediaTool, python_interpreter\n",
    "#\n",
    "# google_search = GoogleSearchTool()\n",
    "# google_knowledge_graph = GoogleKnowledgeGraphTool()\n",
    "# wikidata = WikidataTool(api_wrapper=WikidataAPIWrapper())\n",
    "# wikipedia = WikipediaTool()\n",
    "# tools = [google_search, google_knowledge_graph, wikipedia, wikidata, python_interpreter]\n",
    "#\n",
    "# model_with_tools = model.bind_tools(tools)\n",
    "#\n",
    "# from typing import Literal\n",
    "#\n",
    "# critique_prompt = ChatPromptTemplate.from_messages([\n",
    "# \t(\n",
    "# \t\t\"system\",\n",
    "# \t\t\"You are a reactive agent. Given a question or problem, your job is to select the appropriate tools to answer the question or solve the problem. You should consider the guidance provided by the question planner and error prone points identifier, and the tool results are reliable. If you find the answer from the tool results, you should provide the answer.\"\n",
    "# \t),\n",
    "# \t(\n",
    "# \t\t\"user\",\n",
    "# \t\t\"Question: {question}\"\n",
    "# \t\t\"Guidance: {guidance}\"\n",
    "# \t)\n",
    "# ])\n",
    "#\n",
    "# async def critique_node(state: State):\n",
    "# \tassert state[\"question\"] is not None, \"Question is required\"\n",
    "# \tassert state[\"guidance\"] is not None, \"Guidance is required\"\n",
    "# \tquestion:str = state[\"question\"]\n",
    "# \tguidance:str = state[\"guidance\"]\n",
    "# \tmessages:list[BaseMessage] = []\n",
    "# \tif len(state[\"messages\"]) == 0:\n",
    "# \t\tmessages = critique_prompt.invoke(input={\"question\": question, \"guidance\": guidance}).to_messages()\n",
    "# \t\tcritique:AIMessage = await model_with_tools.ainvoke(input=messages)\n",
    "# \t\tmessages.append(critique)\n",
    "# \telse:\n",
    "# \t\tcritique:AIMessage = await model_with_tools.ainvoke(input=state[\"messages\"])\n",
    "# \t\tmessages.append(critique)\n",
    "# \treturn {\"messages\": messages}\n",
    "#\n",
    "# # Define our tool node\n",
    "# tool_node = ToolNode(tools)\n",
    "# # Define our tool node\n",
    "#\n",
    "#\n",
    "# fusion_prompt = ChatPromptTemplate.from_messages(\n",
    "# \t[\n",
    "# \t\t(\n",
    "# \t\t\t\"placeholder\",\n",
    "# \t\t\t\"{messages}\"\n",
    "# \t\t),\n",
    "# \t\t(\n",
    "# \t\t\t\"user\",\n",
    "# \t\t\t\"Question: {question}\"\n",
    "# \t\t\t\"Now based on the previous information, revise your answer. Use the XML tag <answer></answer> to indicate the final answer part. Do not provide multiple answers in the final answer to increase your chances of getting the answer right. You need to give the answer you think is the most appropriate.\"\n",
    "# \t\t\t\"Do not include any explanations, context, or additional information. Just focus on delivering the exact answer as concisely as possible!!! \"\n",
    "# \t\t\t\"There is no need to answer the question in the form of a complete sentence, just provide the answer in the form of a noun, time, entity, single number, yes or no, etc.\"\n",
    "# \t\t)\n",
    "# \t])\n",
    "# fusion_generator = fusion_prompt | model\n",
    "#\n",
    "# async def fusion_node(state: State) -> State:\n",
    "# \tassert state[\"question\"] is not None, \"Question is required\"\n",
    "# \tassert state[\"guidance\"] is not None, \"Guidance is required\"\n",
    "# \tcritique_messages:Sequence[BaseMessage] = state[\"messages\"][1:]\n",
    "# \tresponse:AIMessage = await fusion_generator.ainvoke(input={\"messages\": critique_messages, \"question\": state[\"question\"]})\n",
    "# \tanswer_matches = re.findall(r\"<answer>(.*?)</answer>\", response.content, re.DOTALL)\n",
    "# \tif answer_matches:\n",
    "# \t\tstate[\"prediction\"] = answer_matches[0]\n",
    "# \telse:\n",
    "# \t\tstate[\"prediction\"] = \"None\"\n",
    "#\n",
    "# \treturn state\n",
    "#\n",
    "# # Define the conditional edge that determines whether to continue or not\n",
    "# def should_continue(state: State) -> Literal[\"fuse\", \"tools\"]:\n",
    "# \tmessages = state[\"messages\"]\n",
    "# \tlast_message = messages[-1]\n",
    "#\n",
    "# \t# If there is no function call, then we finish\n",
    "# \tif last_message.tool_calls:\n",
    "# \t\treturn \"tools\"\n",
    "# \t# Otherwise if there is, we continue\n",
    "# \telse:\n",
    "# \t\treturn \"fuse\"\n",
    "#\n",
    "# def tools_router(state: State) -> Literal[\"fuse\",  \"critique\"]:\n",
    "# \tif state[\"remaining_steps\"] <= 3:\n",
    "# \t\treturn \"fuse\"\n",
    "# \telse:\n",
    "# \t\treturn \"critique\"\n",
    "#\n",
    "#\n",
    "# from langgraph.graph import StateGraph\n",
    "#\n",
    "# workflow = StateGraph(State)\n",
    "# workflow.add_node(\"guide\", guidance_node)\n",
    "# workflow.add_node(\"critique\", critique_node)\n",
    "# workflow.add_node(\"tools\", tool_node)\n",
    "# workflow.add_node(\"fuse\", fusion_node)\n",
    "#\n",
    "# workflow.set_entry_point(\"guide\")\n",
    "# workflow.add_edge(\"guide\", \"critique\")\n",
    "# workflow.add_conditional_edges(\"tools\", tools_router)\n",
    "# workflow.add_conditional_edges(\"critique\", should_continue)\n",
    "# workflow.add_edge(\"fuse\", \"__end__\")\n",
    "#\n",
    "# app = workflow.compile()\n",
    "#\n",
    "# from IPython.display import Image, display\n",
    "#\n",
    "# try:\n",
    "#     display(Image(app.get_graph().draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     pass"
   ],
   "id": "c7c620eadbe22156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# inputs = {**dataset[5], \"messages\": []}\n",
    "# async for event in app.astream(inputs):\n",
    "#     for k, v in event.items():\n",
    "#         if k != \"__end__\":\n",
    "#             print(v)"
   ],
   "id": "55719f2a26bb29e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T10:08:46.610688Z",
     "start_time": "2025-01-22T10:08:46.602333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import ToolMessage, BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from typing import Union\n",
    "# from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "#\n",
    "#\n",
    "# def parse_pretty_repr(pretty_str: str) -> Union[HumanMessage, AIMessage, ToolMessage]:\n",
    "# \t\"\"\"\n",
    "#     解析 pretty_repr 字符串并返回对应的 LangChain 消息对象。\n",
    "#     \"\"\"\n",
    "# \t# 去掉多余的分隔符和空行\n",
    "# \tlines = [line.strip() for line in pretty_str.split(\"\\n\") if line.strip()]\n",
    "#\n",
    "# \tmsg_type = lines[0].strip(\"=\").strip()\n",
    "#\n",
    "# \tcontent = \"\\n\".join(lines[1:])  # 消息的实际内容\n",
    "#\n",
    "# \tif \"Human Message\" in msg_type:\n",
    "# \t\treturn HumanMessage(content=content)\n",
    "# \telif \"Ai Message\" in msg_type:\n",
    "# \t\tif \"Tool Calls\" in content:\n",
    "# \t\t\ttool_calls = re.findall(r\"(\\w+)\\s+\\((.*)\\).*Args:\\s+(.+)\", content, re.DOTALL)\n",
    "# \t\t\tlist_tool_calls = []\n",
    "# \t\t\tfor tool_call in tool_calls:\n",
    "# \t\t\t\tname, call_id = tool_call[:2]\n",
    "# \t\t\t\targs = re.findall(r\"(\\w+): (.+)\", tool_call[2], re.DOTALL)\n",
    "# \t\t\t\targs_dict = {arg[0]: arg[1] for arg in args}\n",
    "# \t\t\t\tlist_tool_calls.append({\"name\": name, \"args\": args_dict, \"id\": call_id})\n",
    "#\n",
    "# \t\t\treturn AIMessage(content=\"\", tool_calls=list_tool_calls)\n",
    "# \t\telse:\n",
    "# \t\t\treturn AIMessage(content=content)\n",
    "# \telif \"Tool Message\" in msg_type:\n",
    "# \t\tname = re.findall(r\"Name: (.*)\\n\", content, re.DOTALL)[0]\n",
    "# \t\tcontent = content.split(f\"{name}\\n\")[-1]\n",
    "# \t\treturn ToolMessage(content=content, name=name, tool_call_id=\"123\")\n",
    "#\n",
    "#\n",
    "# async def construct_messages(item: dict) -> list:\n",
    "# \tmessages = []\n",
    "# \tfor message in item[\"messages\"][1:]:\n",
    "# \t\tmessages.append(parse_pretty_repr(message))\n",
    "# \treturn messages\n",
    "#\n",
    "def construct_messages(messages_dict: list[dict]) -> list[BaseMessage]:\n",
    "\tmessages = []\n",
    "\tfor message_dict in messages_dict:\n",
    "\t\tif message_dict[\"type\"] == \"human\":\n",
    "\t\t\tmessages.append(HumanMessage(**message_dict))\n",
    "\t\telif message_dict[\"type\"] == \"ai\":\n",
    "\t\t\tmessages.append(AIMessage(**message_dict))\n",
    "\t\telif message_dict[\"type\"] == \"tool\":\n",
    "\t\t\tmessages.append(ToolMessage(**message_dict))\n",
    "\t\t# elif message_dict[\"type\"] == \"system\":\n",
    "\t\t# \tmessages.append(SystemMessage(**message_dict))\n",
    "\n",
    "\treturn messages\n",
    "\n",
    "prediction_prompt = ChatPromptTemplate.from_messages(\n",
    "\t[\n",
    "\t\t(\n",
    "\t\t\t\"placeholder\",\n",
    "\t\t\t\"{messages}\"\n",
    "\t\t),\n",
    "\t\t(\n",
    "\t\t\t\"user\",\n",
    "\t\t\t\"Question: {question}\"\n",
    "\t\t\t\"Now based on the previous information, revise your answer. Use the XML tag <answer></answer> to indicate the final answer part. Do not provide multiple answers in the final answer to increase your chances of getting the answer right. You need to give the answer you think is the most appropriate.\"\n",
    "\t\t\t\"Do not include any explanations, context, or additional information. Just focus on delivering the exact answer as concisely as possible!!! \"\n",
    "\t\t\t\"There is no need to answer the question in the form of a complete sentence, just provide the answer in the form of a noun, time, entity, single number, yes or no, etc.\"\n",
    "\t\t)\n",
    "\t])\n",
    "prediction_generator = prediction_prompt | model\n"
   ],
   "id": "32bf276ff4d80ddd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:08:28.168426Z",
     "start_time": "2025-01-23T04:08:28.157473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import message_to_dict\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import nest_asyncio\n",
    "\n",
    "# 配置logger\n",
    "logging.basicConfig(\n",
    "\tlevel=logging.ERROR,  # 设置日志级别\n",
    "\tformat='%(asctime)s - %(levelname)s - %(message)s',  # 设置日志格式\n",
    "\thandlers=[\n",
    "\t\tlogging.FileHandler(\"inference.log\"),  # 将日志输出到文件\n",
    "\t\tlogging.StreamHandler()  # 也输出到控制台\n",
    "\t]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"InferenceLogger\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "results = []\n",
    "batch_size = 100\n",
    "\n",
    "async def process(item):\n",
    "\ttry:\n",
    "\t\tif not item.get(\"messages\"):\n",
    "\t\t\treturn {**item, \"prediction\": \"None\"}\n",
    "\t\tmessages = construct_messages(item[\"messages\"])\n",
    "\t\tresponse: AIMessage = await prediction_generator.ainvoke(input={\"messages\": messages, \"question\": item[\"question\"]})\n",
    "\t\tanswer_matches = re.findall(r\"<answer>(.*?)</answer>\", response.content, re.DOTALL)\n",
    "\t\tif answer_matches:\n",
    "\t\t\tprediction = answer_matches[0]\n",
    "\t\telse:\n",
    "\t\t\tprediction = \"None\"\n",
    "\t\treturn {**item, \"response\": message_to_dict(response), \"prediction\": prediction}\n",
    "\texcept Exception as e:\n",
    "\t\tlogger.error(f\"Error processing item: {item}. Error: {e}\")\n",
    "\t\traise\n",
    "\n",
    "# async def process(item):\n",
    "# \ttry:\n",
    "# \t\titem[\"messages\"] = construct_messages(item[\"messages\"])\n",
    "# \t\tdel item[\"prediction\"]\n",
    "# \t\tstate = await app.ainvoke({**item}, config={\"recursion_limit\": 18})\n",
    "# \t\tstate[\"messages\"] = messages_to_dict(state[\"messages\"])\n",
    "# \t\tlogger.info(f\"Processed item: {item}\")\n",
    "# \t\treturn {**item, **state}\n",
    "# \texcept Exception as e:\n",
    "# \t\tlogger.error(f\"Error processing item: {item}. Error: {e}\")\n",
    "# \t\treturn {**item, \"prediction\": \"None\"}\n",
    "\n",
    "\n",
    "\n",
    "async def self_improve_inference() -> None:\n",
    "\terror_indices = []  # 用于记录包含 \"ERROR\" 的条目索引\n",
    "\n",
    "\t# 读取已有结果或初始化文件\n",
    "\tif os.path.exists(save_results_path):\n",
    "\t\tlogger.info(f\"Loading existing results from {save_results_path}\")\n",
    "\t\twith open(save_results_path, 'r') as file:\n",
    "\t\t\tfor idx, line in enumerate(file):\n",
    "\t\t\t\tresult = json.loads(line)\n",
    "\t\t\t\tresults.append(result)\n",
    "\t\t\t\t# 检查是否存在 \"prediction: ERROR\"\n",
    "\t\t\t\tif \"None\" == result.get(\"prediction\"):\n",
    "\t\t\t\t\terror_indices.append(idx)\n",
    "\telse:\n",
    "\t\tfolder_path = os.path.dirname(save_results_path)\n",
    "\t\tos.makedirs(folder_path, exist_ok=True)\n",
    "\t\tlogger.info(f\"Created directory for results: {folder_path}\")\n",
    "\n",
    "\t# 重新推理错误的数据\n",
    "\tif error_indices:\n",
    "\t\tlogger.warning(f\"Found {len(error_indices)} ERROR entries. Retrying inference...\")\n",
    "\t\terror_data = [dataset[idx] for idx in error_indices]\n",
    "\t\tnew_results = await tqdm_asyncio.gather(*(process(item) for item in error_data))\n",
    "\t\t# 更新原始结果\n",
    "\t\tfor i, new_result in zip(error_indices, new_results):\n",
    "\t\t\tresults[i] = new_result\n",
    "\n",
    "\n",
    "\tfor idx in range(len(results), len(dataset), batch_size):\n",
    "\t\tbatch = dataset[idx: min(idx + batch_size, len(dataset))]\n",
    "\t\tbatch_results = await tqdm_asyncio.gather(*(process(item) for item in batch))\n",
    "\t\tresults.extend(batch_results)\n",
    "\n",
    "\t\tlogger.info(f\"Processed batch starting at index {idx}\")\n",
    "\n",
    "\t\t# 保存结果\n",
    "\t\twith open(save_results_path, 'w') as file:\n",
    "\t\t\tfor result in results:\n",
    "\t\t\t\tfile.write(json.dumps(result) + \"\\n\")\n",
    "\t\tlogger.info(f\"Saved results to {save_results_path}\")\n"
   ],
   "id": "84f12d6e0ec85f0c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:16:01.822375Z",
     "start_time": "2025-01-23T04:08:29.110791Z"
    }
   },
   "cell_type": "code",
   "source": "await self_improve_inference()",
   "id": "cf8aad5c193ddba2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      " 20%|██        | 1/5 [00:01<00:04,  1.16s/it]\u001B[A\n",
      " 80%|████████  | 4/5 [00:01<00:00,  2.89it/s]\u001B[A\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.70it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  3%|▎         | 3/100 [00:02<01:34,  1.03it/s]\u001B[A\n",
      " 10%|█         | 10/100 [00:03<00:21,  4.23it/s]\u001B[A\n",
      " 14%|█▍        | 14/100 [00:03<00:13,  6.37it/s]\u001B[A\n",
      " 19%|█▉        | 19/100 [00:03<00:09,  9.00it/s]\u001B[A\n",
      " 26%|██▌       | 26/100 [00:03<00:05, 14.70it/s]\u001B[A\n",
      " 31%|███       | 31/100 [00:03<00:03, 17.90it/s]\u001B[A\n",
      " 44%|████▍     | 44/100 [00:03<00:01, 32.67it/s]\u001B[A\n",
      " 52%|█████▏    | 52/100 [00:03<00:01, 40.07it/s]\u001B[A\n",
      " 59%|█████▉    | 59/100 [00:03<00:00, 43.89it/s]\u001B[A\n",
      " 66%|██████▌   | 66/100 [00:04<00:01, 21.86it/s]\u001B[A\n",
      " 71%|███████   | 71/100 [00:04<00:01, 21.40it/s]\u001B[A\n",
      " 77%|███████▋  | 77/100 [00:05<00:00, 25.74it/s]\u001B[A\n",
      " 82%|████████▏ | 82/100 [00:06<00:01, 11.67it/s]\u001B[A\n",
      " 95%|█████████▌| 95/100 [00:06<00:00, 20.78it/s]\u001B[A\n",
      " 95%|█████████▌| 95/100 [00:17<00:00, 20.78it/s]\u001B[A\n",
      "100%|██████████| 100/100 [01:02<00:00,  1.60it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▏         | 2/100 [00:02<02:12,  1.35s/it]\u001B[A\n",
      " 17%|█▋        | 17/100 [00:02<00:10,  7.67it/s]\u001B[A\n",
      " 21%|██        | 21/100 [00:03<00:08,  9.40it/s]\u001B[A\n",
      " 25%|██▌       | 25/100 [00:03<00:06, 11.75it/s]\u001B[A\n",
      " 33%|███▎      | 33/100 [00:03<00:03, 18.61it/s]\u001B[A\n",
      " 44%|████▍     | 44/100 [00:03<00:01, 28.83it/s]\u001B[A\n",
      " 53%|█████▎    | 53/100 [00:03<00:01, 36.38it/s]\u001B[A\n",
      " 60%|██████    | 60/100 [00:03<00:01, 38.25it/s]\u001B[A\n",
      " 77%|███████▋  | 77/100 [00:03<00:00, 59.30it/s]\u001B[A\n",
      " 86%|████████▌ | 86/100 [00:04<00:00, 54.88it/s]\u001B[A\n",
      " 94%|█████████▍| 94/100 [00:04<00:00, 33.38it/s]\u001B[A\n",
      "100%|██████████| 100/100 [00:05<00:00, 19.07it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▏         | 2/100 [00:02<01:57,  1.20s/it]\u001B[A\n",
      "  8%|▊         | 8/100 [00:02<00:22,  4.10it/s]\u001B[A\n",
      " 11%|█         | 11/100 [00:02<00:16,  5.41it/s]\u001B[A\n",
      " 15%|█▌        | 15/100 [00:02<00:10,  8.32it/s]\u001B[A\n",
      " 23%|██▎       | 23/100 [00:02<00:04, 15.89it/s]\u001B[A\n",
      " 34%|███▍      | 34/100 [00:03<00:02, 28.33it/s]\u001B[A\n",
      " 46%|████▌     | 46/100 [00:03<00:01, 42.52it/s]\u001B[A\n",
      " 55%|█████▌    | 55/100 [00:03<00:00, 49.90it/s]\u001B[A\n",
      " 63%|██████▎   | 63/100 [00:04<00:02, 17.44it/s]\u001B[A\n",
      " 73%|███████▎  | 73/100 [00:04<00:01, 24.20it/s]\u001B[A\n",
      " 95%|█████████▌| 95/100 [00:04<00:00, 44.80it/s]\u001B[A\n",
      " 95%|█████████▌| 95/100 [00:20<00:00, 44.80it/s]\u001B[A\n",
      "100%|██████████| 100/100 [01:02<00:00,  1.59it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  5%|▌         | 5/100 [00:02<00:49,  1.91it/s]\u001B[A\n",
      "  7%|▋         | 7/100 [00:02<00:32,  2.84it/s]\u001B[A\n",
      "  9%|▉         | 9/100 [00:02<00:22,  3.98it/s]\u001B[A\n",
      " 12%|█▏        | 12/100 [00:02<00:14,  6.15it/s]\u001B[A\n",
      " 14%|█▍        | 14/100 [00:03<00:11,  7.55it/s]\u001B[A\n",
      " 23%|██▎       | 23/100 [00:03<00:04, 18.44it/s]\u001B[A\n",
      " 30%|███       | 30/100 [00:03<00:02, 25.70it/s]\u001B[A\n",
      " 37%|███▋      | 37/100 [00:03<00:01, 31.98it/s]\u001B[A\n",
      " 50%|█████     | 50/100 [00:03<00:01, 47.78it/s]\u001B[A\n",
      " 58%|█████▊    | 58/100 [00:03<00:00, 53.03it/s]\u001B[A\n",
      " 65%|██████▌   | 65/100 [00:04<00:01, 22.50it/s]\u001B[A\n",
      " 78%|███████▊  | 78/100 [00:04<00:00, 34.37it/s]\u001B[A\n",
      " 96%|█████████▌| 96/100 [00:04<00:00, 54.33it/s]\u001B[A\n",
      " 96%|█████████▌| 96/100 [00:17<00:00, 54.33it/s]\u001B[A\n",
      "100%|██████████| 100/100 [01:01<00:00,  1.62it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  1%|          | 1/100 [00:02<04:36,  2.80s/it]\u001B[A\n",
      " 11%|█         | 11/100 [00:02<00:17,  5.19it/s]\u001B[A\n",
      " 17%|█▋        | 17/100 [00:03<00:10,  7.75it/s]\u001B[A\n",
      " 22%|██▏       | 22/100 [00:03<00:07, 10.33it/s]\u001B[A\n",
      " 34%|███▍      | 34/100 [00:03<00:03, 20.21it/s]\u001B[A\n",
      " 47%|████▋     | 47/100 [00:03<00:01, 31.78it/s]\u001B[A\n",
      " 55%|█████▌    | 55/100 [00:03<00:01, 31.12it/s]\u001B[A\n",
      " 61%|██████    | 61/100 [00:04<00:01, 29.65it/s]\u001B[A\n",
      " 66%|██████▌   | 66/100 [00:04<00:01, 31.07it/s]\u001B[A\n",
      " 74%|███████▍  | 74/100 [00:04<00:00, 37.23it/s]\u001B[A\n",
      " 80%|████████  | 80/100 [00:05<00:01, 13.53it/s]\u001B[A\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.73it/s][A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  1%|          | 1/100 [00:02<04:16,  2.59s/it]\u001B[A\n",
      " 11%|█         | 11/100 [00:02<00:16,  5.40it/s]\u001B[A\n",
      " 17%|█▋        | 17/100 [00:02<00:09,  8.50it/s]\u001B[A\n",
      " 21%|██        | 21/100 [00:03<00:07, 10.95it/s]\u001B[A\n",
      " 25%|██▌       | 25/100 [00:03<00:05, 13.83it/s]\u001B[A\n",
      " 35%|███▌      | 35/100 [00:03<00:02, 24.20it/s]\u001B[A\n",
      " 48%|████▊     | 48/100 [00:03<00:01, 40.09it/s]\u001B[A\n",
      " 56%|█████▌    | 56/100 [00:03<00:01, 40.78it/s]\u001B[A\n",
      " 63%|██████▎   | 63/100 [00:03<00:01, 31.96it/s]\u001B[A\n",
      " 69%|██████▉   | 69/100 [00:05<00:02, 13.71it/s]\u001B[A\n",
      " 83%|████████▎ | 83/100 [00:05<00:00, 23.20it/s]\u001B[A\n",
      " 97%|█████████▋| 97/100 [00:05<00:00, 32.74it/s]\u001B[A\n",
      " 97%|█████████▋| 97/100 [00:18<00:00, 32.74it/s]\u001B[A\n",
      " 99%|█████████▉| 99/100 [01:03<00:02,  2.19s/it]\u001B[A\n",
      "100%|██████████| 100/100 [01:04<00:00,  1.56it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  1%|          | 1/100 [00:02<04:29,  2.72s/it]\u001B[A\n",
      " 15%|█▌        | 15/100 [00:02<00:11,  7.09it/s]\u001B[A\n",
      " 21%|██        | 21/100 [00:03<00:07, 10.44it/s]\u001B[A\n",
      " 27%|██▋       | 27/100 [00:03<00:05, 13.05it/s]\u001B[A\n",
      " 36%|███▌      | 36/100 [00:03<00:03, 19.34it/s]\u001B[A\n",
      " 41%|████      | 41/100 [00:03<00:02, 21.92it/s]\u001B[A\n",
      " 51%|█████     | 51/100 [00:03<00:01, 31.45it/s]\u001B[A\n",
      " 58%|█████▊    | 58/100 [00:03<00:01, 35.79it/s]\u001B[A\n",
      " 64%|██████▍   | 64/100 [00:04<00:01, 31.94it/s]\u001B[A\n",
      " 77%|███████▋  | 77/100 [00:04<00:00, 48.25it/s]\u001B[A\n",
      " 85%|████████▌ | 85/100 [00:05<00:00, 20.18it/s]\u001B[A\n",
      " 98%|█████████▊| 98/100 [00:05<00:00, 29.82it/s]\u001B[A\n",
      " 98%|█████████▊| 98/100 [00:24<00:00, 29.82it/s]\u001B[A\n",
      "100%|██████████| 100/100 [01:01<00:00,  1.62it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▏         | 2/100 [00:03<02:38,  1.61s/it]\u001B[A\n",
      " 11%|█         | 11/100 [00:03<00:20,  4.34it/s]\u001B[A\n",
      " 15%|█▌        | 15/100 [00:03<00:14,  5.96it/s]\u001B[A\n",
      " 18%|█▊        | 18/100 [00:03<00:10,  7.64it/s]\u001B[A\n",
      " 25%|██▌       | 25/100 [00:03<00:05, 13.00it/s]\u001B[A\n",
      " 30%|███       | 30/100 [00:04<00:04, 15.52it/s]\u001B[A\n",
      " 39%|███▉      | 39/100 [00:04<00:02, 24.78it/s]\u001B[A\n",
      " 45%|████▌     | 45/100 [00:04<00:01, 30.14it/s]\u001B[A\n",
      " 51%|█████     | 51/100 [00:04<00:02, 21.22it/s]\u001B[A\n",
      " 56%|█████▌    | 56/100 [00:05<00:02, 18.08it/s]\u001B[A\n",
      " 65%|██████▌   | 65/100 [00:05<00:01, 26.73it/s]\u001B[A\n",
      " 90%|█████████ | 90/100 [00:05<00:00, 59.03it/s]\u001B[A\n",
      " 90%|█████████ | 90/100 [00:22<00:00, 59.03it/s]\u001B[A\n",
      " 99%|█████████▉| 99/100 [01:01<00:01,  1.53s/it]\u001B[A\n",
      "100%|██████████| 100/100 [01:02<00:00,  1.60it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001B[A\n",
      "  4%|▍         | 4/100 [00:02<01:07,  1.42it/s]\u001B[A\n",
      " 18%|█▊        | 18/100 [00:02<00:10,  8.02it/s]\u001B[A\n",
      " 25%|██▌       | 25/100 [00:03<00:07, 10.69it/s]\u001B[A\n",
      " 31%|███       | 31/100 [00:03<00:04, 14.26it/s]\u001B[A\n",
      " 38%|███▊      | 38/100 [00:03<00:03, 18.54it/s]\u001B[A\n",
      " 44%|████▍     | 44/100 [00:03<00:02, 22.99it/s]\u001B[A\n",
      " 50%|█████     | 50/100 [00:03<00:01, 27.09it/s]\u001B[A\n",
      " 55%|█████▌    | 55/100 [00:04<00:02, 21.13it/s]\u001B[A\n",
      " 60%|██████    | 60/100 [00:04<00:01, 24.53it/s]\u001B[A\n",
      " 65%|██████▌   | 65/100 [00:04<00:01, 27.84it/s]\u001B[A\n",
      " 70%|███████   | 70/100 [00:05<00:02, 10.38it/s]\u001B[A\n",
      " 88%|████████▊ | 88/100 [00:05<00:00, 23.43it/s]\u001B[A\n",
      " 97%|█████████▋| 97/100 [00:05<00:00, 27.52it/s]\u001B[A\n",
      " 97%|█████████▋| 97/100 [00:20<00:00, 27.52it/s]\u001B[A\n",
      " 99%|█████████▉| 99/100 [01:02<00:02,  2.46s/it]\u001B[A\n",
      "100%|██████████| 100/100 [01:03<00:00,  1.58it/s]\u001B[A\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
