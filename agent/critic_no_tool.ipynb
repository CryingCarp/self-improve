{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"critic no tools\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import message\n",
    "from typing import Literal\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from operator import add\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage, merge_message_runs\n",
    "dataset_name = \"hotpot_qa\"\n",
    "\n",
    "MAX_ITERATIONS = 4\n",
    "\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    messages: Annotated[list, add]\n",
    "    predictions: Annotated[list[str], add]\n",
    "    iteration: int\n",
    "\n",
    "critic_prompt = HumanMessage(content=(\"Inspect the previous messages and identify any potential issues or errors. \"\n",
    "                                    \"Check the Plausibility、 Truthfulness、 Correctness of your previous answer. \"\n",
    "                                    \"Your response should be short、 direct and concise.\"))\n",
    "\n",
    "if dataset_name == \"hotpot_qa\":\n",
    "    revise_prompt = HumanMessage(content=(\"Based on the previous criqique, revise your answer. Remember your FINAL ANSWER should be clear and concise.(a single number or phrases, not a sentence!) \"\n",
    "                                      \"Your response should follow the original format. For following example:\\n\\n\"\n",
    "                                      \"Proposed Answer: Let's think step by step. ...\\n\"\n",
    "                                      \"So the FINAL ANSWER is: <FINAL ANSWER>\\n\"))\n",
    "    llm = ChatOpenAI(temperature=0, base_url=\"https://api.chsdw.top/v1\", model=\"gpt-4o-mini\")\n",
    "elif dataset_name == \"gsm8k\":\n",
    "    revise_prompt = HumanMessage(content=(\"Based on the previous criqique, revise your answer.  \"\n",
    "                                      \"Your response should follow the original format. Here is a better solution:\\n\\n\"))\n",
    "    llm = ChatOpenAI(temperature=0.5, base_url=\"https://api.chsdw.top/v1\", model=\"gpt-4o-mini\")\n",
    "elif dataset_name == \"toxicity\":\n",
    "    revise_prompt = HumanMessage(content=(\"Based on the previous criqique, revise your answer. Remember your FINAL ANSWER should be clear and concise.(a single number or phrases, not a sentence!) \"\n",
    "                                      \"Your response should follow the original format. For following example:\\n\\n\"\n",
    "                                      \"Proposed Answer: Let's think step by step. ...\\n\"\n",
    "                                      \"So the FINAL ANSWER is: <FINAL ANSWER>\\n\"))\n",
    "    \n",
    "def criticize(state):\n",
    "    try:\n",
    "        messages = [critic_prompt]\n",
    "        critique = llm.invoke(state[\"messages\"] + messages)\n",
    "        messages.append(AIMessage(**critique.dict(exclude={\"type\", \"name\"})))\n",
    "    except:\n",
    "        return {\n",
    "        \"iteration\":state[\"iteration\"] + 1 \n",
    "        } \n",
    "    return {\n",
    "    \"messages\":messages,\n",
    "    \"iteration\":state[\"iteration\"] + 1 \n",
    "    }\n",
    "\n",
    "def react(state):\n",
    "    try:\n",
    "        question_message = HumanMessage(content=state[\"input\"])\n",
    "        messages = merge_message_runs(revise_prompt, question_message)\n",
    "        result = llm.invoke(state[\"messages\"] + [messages])\n",
    "        result_message = AIMessage(**result.dict(exclude={\"type\", \"name\"}))\n",
    "    except:\n",
    "        return{\n",
    "            \"messages\": [SystemMessage(content=\"Sorry, I have trouble understanding your answer. Please try again.\")],\n",
    "            \"predictions\": [],\n",
    "        }\n",
    "    return {\n",
    "        \"messages\": [revise_prompt, HumanMessage(content=state[\"input\"]), result_message],\n",
    "        \"predictions\": [result_message.content.split(\"FINAL ANSWER :\")[-1].strip()],\n",
    "    }\n",
    "\n",
    "# Either agent can decide to end\n",
    "from typing import Literal\n",
    "\n",
    "def should_end(state) -> Literal[\"critic\", \"__end__\"]:\n",
    "    if state[\"iteration\"] == 4 or len(state[\"predictions\"]) > 1 and state[\"predictions\"][-1] == state[\"predictions\"][-2]:\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        return \"critic\"\n",
    "    \n",
    "def should_criticize(state) -> Literal[\"react\"]:\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        return \"call_tools\"\n",
    "    else:\n",
    "        return \"react\"\n",
    "    \n",
    "\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_improve import *\n",
    "\n",
    "dataset_name = \"hotpot_qa\"\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"critic\", criticize)\n",
    "builder.add_node(\"react\", react)\n",
    "\n",
    "builder.add_edge(START, \"critic\")\n",
    "builder.add_edge(\"critic\", \"react\")\n",
    "\n",
    "builder.add_conditional_edges(\"react\", should_end)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并行处理\n",
    "import asyncio\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "async def reflect(item, graph, dataset_name:str=\"hotpot_qa\") -> str:\n",
    "    if dataset_name == \"hotpot_qa\":\n",
    "        input={\"messages\": [HumanMessage(content=item[\"question\"]), HumanMessage(content=item[\"prediction\"])], \"iteration\": 0}\n",
    "    elif dataset_name == \"toxicity\":\n",
    "        messages = HumanMessage(content=item[\"prompt\"][\"text\"])\n",
    "    try:\n",
    "        result = await graph.ainvoke(input=input)\n",
    "        return {**item, \"predictions\": result[\"predictions\"]}\n",
    "    except Exception:\n",
    "        return {**item, \"predictions\": [\"None\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotpot QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"hotpot_qa\"\n",
    "mode = \"critic_no_tool\"\n",
    "num_test_sample = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'question', 'answer', 'prediction'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=f\"../output/{dataset_name}/200_cot.jsonl\", split=\"train\")\n",
    "if num_test_sample > 0:\n",
    "    dataset = dataset.select(range(num_test_sample))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm_asyncio\n",
    "results = await tqdm_asyncio.gather(*(reflect(item, graph, dataset_name) for item in dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = f\"/Users/ariete/Projects/self-improve/output/{dataset_name}\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "with open (\"/Users/ariete/Projects/self-improve/output/{}/{}_{}.jsonl\".format(dataset_name, num_test_sample, mode), \"w\") as f:\n",
    "    for idx, result in enumerate(results):\n",
    "        f.write(json.dumps({\"idx\": idx, \"question\": dataset[idx][\"question\"], \"answer\":dataset[idx][\"answer\"], \"predictions\": results[idx][\"predictions\"]}) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfimprove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
