{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_core.outputs import LLMResult\n",
    "from agent.utils.tools.google_search import GoogleSearchTool\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "\n",
    "from agent.utils.loader import load_prompt, load_processed_data\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from agent.utils.tools.interpreter_api import safe_execute\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_name = 'gsm8k'\n",
    "mode = \"critic\"\n",
    "basemode = \"pot\"\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "num_samples = -1\n",
    "top_p = 0.95\n",
    "temperature = 0\n",
    "n=1\n",
    "seed = 42\n",
    "batch_size = 100\n",
    "if \"gpt-4o-mini\" in model:\n",
    "    processed_data_path = f\"../../../output/inference/gpt-4o-mini/{dataset_name}/{basemode}/num_samples_{num_samples}_top_p_{top_p}_temperature_0_seed_{seed}.jsonl\"\n",
    "else:\n",
    "\tprocessed_data_path = f\"../../../output/inference/{model}/{dataset_name}/{basemode}/num_samples_{num_samples}_top_p-{top_p}_temperature_0_seed_{seed}.jsonl\"\n",
    "save_results_path = f\"../../../output/inference/{model}/{dataset_name}/{mode}/num_samples_{num_samples}_top_p_{top_p}_temperature_{temperature}_seed_{seed}.jsonl\"\n",
    "prompt = load_prompt(dataset_name=dataset_name, mode=mode)\n",
    "dataset = load_processed_data(dataset_name=dataset_name, file_path=processed_data_path)\n",
    "if num_samples > 0:\n",
    "\tdataset = dataset.select(range(num_samples))\n",
    "llm = ChatOpenAI(model=model, top_p=top_p, n=n, temperature=temperature, base_url=\"https://api.chsdw.top/v1\", seed=seed)\n",
    "\n",
    "prompt.pretty_print()\n",
    "print(dataset[2])"
   ],
   "id": "91d15c1b88620b2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "from utils import remove_comment, floatify_ans, finqa_equal\n",
    "MAX_ITERATION = 7\n",
    "async def critic_iter(sample: dict) -> dict:\n",
    "\timport re\n",
    "\tdef extract_code(text: str):\n",
    "\t\tpattern = r'```python(.*?)```'\n",
    "\t\tmatch = re.search(pattern, text, re.DOTALL)\n",
    "\t\tif match:\n",
    "\t\t\treturn match.group(1).strip()\n",
    "\t\telse:\n",
    "\t\t\treturn text\n",
    "\tsample['answer'] = sample['answer'].replace('\\n', '')\n",
    "\tfor itr in range(1, 4):\n",
    "\t\tif itr == 1:\n",
    "\t\t\tprediction, report = safe_execute(sample[\"code\"])\n",
    "\t\t\t# print(\"Is initial program correct:\", sample['answer'] == prediction)\n",
    "\t\t\tsample['prediction'] = [prediction]\n",
    "\t\t\tsample['report'] = [report]\n",
    "\t\t\tsample['code'] = [sample['code']]\n",
    "\t\t# print(\"\\n\" + \"-\" * 20, \"iteration\", itr, \"-\" * 20)\n",
    "\t\t\n",
    "\t\t# criticize latest answer that is not \"None\"\n",
    "\t\tbase_idx = itr - 1\n",
    "\t\twhile base_idx > 0 and sample['prediction'][base_idx] is None:\n",
    "\t\t\tbase_idx -= 1\n",
    "\n",
    "\t\tprevious_code = remove_comment(sample['code'][base_idx])\n",
    "\n",
    "\t\t# construct prompt\n",
    "\t\tcontext = f\"Question: {sample['question']}\\n\"\n",
    "\t\tcontext += f\"```python\\n{previous_code}\\n```\\n\"\n",
    "\t\tcontext += f\"Execution: {sample['report'][base_idx]}\\n\"\n",
    "\t\tcontext += f\"Output: answer = {floatify_ans(sample['prediction'][base_idx])}\\n\"\n",
    "\t\tcontext += \"\\nWhat's the problem with the above code?\\n\\n\"\n",
    "\t\tprompt_critic = prompt.invoke(input={\"context\": context})\n",
    "\n",
    "\t\t# verify previous code\n",
    "\t\ttry:\n",
    "\t\t\tresult = await llm.ainvoke(prompt_critic, stop=[\"Here's\", \"---\"])\n",
    "\t\texcept:\n",
    "\t\t\tresult = None\n",
    "\t\t\t\n",
    "\t\tcontext += result.content if result else \"\"\n",
    "\t\t# print(context)\n",
    "\t\t\n",
    "\t\t# if context not end with a \"\\n\", add \"\\n\"\n",
    "\t\tif context and context[-1] != \"\\n\":\n",
    "\t\t\tcontext += \"\\n\"\n",
    "\n",
    "\t\t# print(\"=\"*10, \"生成新代码\", \"=\"*10)\n",
    "\t\t# generate new code\n",
    "\t\tcontext += \"Here's a better solution:\\n\"\n",
    "\t\tprompt_critic = prompt.invoke(input={\"context\": context})\n",
    "\t\t# print(context)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tresult = await llm.ainvoke(prompt_critic, stop=[\"```\\n\", \"---\"])\n",
    "\t\texcept:\n",
    "\t\t\tresult = None\n",
    "\n",
    "\t\t# print(result.content)\n",
    "\t\t# excute new code\n",
    "\t\tcode = extract_code(result.content) if result else \"\"\n",
    "\t\tprediction, report = safe_execute(code)\n",
    "\t\tprediction = floatify_ans(prediction)\n",
    "\t\tcorrected = True\n",
    "\t\t# print(\"Execution:\", report)\n",
    "\t\t# print(\"Output: answer =\", prediction)\n",
    "\n",
    "\t\tif code.strip() == sample['code'][base_idx].strip(): # no correction\n",
    "\t\t\tcorrected = False\n",
    "\t\t\tcode = sample['code'][base_idx]\n",
    "\t\t\treport = sample['report'][base_idx]\n",
    "\t\t\tprediction = sample['prediction'][base_idx]\n",
    "\n",
    "\t\t# append new result\n",
    "\t\tsample['code'].append(code)\n",
    "\t\tsample['report'].append(report)\n",
    "\t\tsample['prediction'].append(prediction)\n",
    "\t\tis_correct = finqa_equal(prediction, sample['answer'])\n",
    "\n",
    "\t\t# print(\"Gold Answer:\", sample['answer'])\n",
    "\t\t# print(\"Corrected:\", \"Yes\" if corrected else \"No\")\n",
    "\t\t# print(\"Is correct:\", is_correct)\n",
    "\t\n",
    "\treturn sample\n",
    "\n",
    "async def critic() -> None:\n",
    "\tresults = []\n",
    "\tif os.path.exists(save_results_path):\n",
    "\t    with open(save_results_path, 'r') as file:\n",
    "\t        for line in file:\n",
    "\t            results.append(json.loads(line))\n",
    "\telse:\n",
    "\t\tfolder_path = os.path.dirname(save_results_path)\n",
    "\t\tos.makedirs(folder_path, exist_ok=True)\n",
    "\t\n",
    "\tfor idx in tqdm(range(len(results), dataset.num_rows, batch_size)):\n",
    "\t\tbatch = dataset.select(range(idx, min(idx+batch_size, dataset.num_rows)))\n",
    "\t\tresults.extend(await tqdm_asyncio.gather(*(critic_iter(item) for item in batch)))\n",
    "\t\twith open(save_results_path, 'w') as file:\n",
    "\t\t\tfor result in results:\n",
    "\t\t\t\tfile.write(json.dumps(result) + \"\\n\")\n"
   ],
   "id": "14a95f81eeb74149",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await critic()",
   "id": "3c14b8855514ea24",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
