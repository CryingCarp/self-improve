{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T06:38:28.435894Z",
     "start_time": "2024-12-13T06:38:25.757860Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from agent.utils.loader import load_processed_data\n",
    "from agent.utils.tools import python_interpreter\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:38:29.638373Z",
     "start_time": "2024-12-13T06:38:28.443559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = 'tabmwp'\n",
    "mode = \"self-improve\"\n",
    "base_mode = \"pot\"\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "num_samples = -1\n",
    "top_p = 0.95\n",
    "temperature = 0\n",
    "seed = 42\n",
    "batch_size = 100\n",
    "if base_mode == \"pot\":\n",
    "\tprocessed_data_path = f\"../../output/inference/gpt-4o-mini/{dataset_name}/pot/num_samples_-1_top_p_0.95_temperature_0_seed_42.jsonl\"\n",
    "else:\n",
    "\tprocessed_data_path = f\"../../data/processed_data/{dataset_name}.jsonl\"\n",
    "dataset = load_processed_data(dataset_name=dataset_name, file_path=processed_data_path)\n",
    "if dataset_name == \"tabmwp\":\n",
    "\tdataset = dataset.map(lambda x: {\"question\": f\"{x[\"context\"]}\\n{x[\"question\"]}\", \"initial_answer\": f\"```python\\n{x[\"code\"]}```\"})\n",
    "\tdataset = dataset.remove_columns([\"prediction\", \"code\"])\n",
    "elif dataset_name in [\"gsmhard\", \"gsm8k\"]:\n",
    "\tdataset = dataset.rename_column(\"code\", \"initial_answer\")\n",
    "\tdataset = dataset.remove_columns(\"prediction\")\n",
    "\tdataset = dataset.map(lambda x: {\"question\": f\"For the following math question, just focus on the calculation process without considering any realistic factors.\\n{x[\"question\"]}\", \"initial_answer\": f\"```python\\n{x[\"initial_answer\"]}```\"})\n",
    "elif dataset_name == \"math\":\n",
    "\tfrom agent.utils.math_util import last_boxed_only_string\n",
    "\tdef remove_boxed(s):\n",
    "\t\tleft = \"\\\\boxed{\"\n",
    "\t\ttry:\n",
    "\t\t\tassert s[:len(left)] == left\n",
    "\t\t\tassert s[-1] == \"}\"\n",
    "\t\t\treturn s[len(left):-1]\n",
    "\t\texcept:\n",
    "\t\t\treturn None\n",
    "\tdataset = dataset.map(lambda x: {\"answer\": remove_boxed(last_boxed_only_string(x[\"solution\"]))})\n",
    "if num_samples > 0:\n",
    "\tdataset = dataset.select(range(num_samples))\n",
    "print(dataset[10])\n",
    "# calculator = calculator\n",
    "python_interpreter = python_interpreter"
   ],
   "id": "3654c0930830c6c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Read the following table regarding \"None\" and then answer a question.\\n\\nfine gravel | $2 per lb\\npebbles | $3 per lb\\nblack sand | $3 per lb\\nrocks | $3 per lb\\ncoarse gravel | $3 per lb\\nwhite sand | $5 per lb', 'question': 'Read the following table regarding \"None\" and then answer a question.\\n\\nfine gravel | $2 per lb\\npebbles | $3 per lb\\nblack sand | $3 per lb\\nrocks | $3 per lb\\ncoarse gravel | $3 per lb\\nwhite sand | $5 per lb\\nBrenda purchased 1.1 pounds of coarse gravel. What was the total cost?', 'answer': '3.30', 'ques_type': 'free_text', 'choices': None, 'initial_answer': '```python\\n# Python code, return answer \\nprice_per_pound_coarse_gravel = 3 \\npounds_purchased = 1.1 \\n# Calculate total cost\\ntotal_cost = price_per_pound_coarse_gravel * pounds_purchased\\nanswer = total_cost\\nprint(answer)```'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:38:29.795593Z",
     "start_time": "2024-12-13T06:38:29.760115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from typing import  List\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import   END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class Step(TypedDict):\n",
    "\tstep: str\n",
    "\terror_prone_points: str\n",
    "\tevidence: str\n",
    "\tresult: str\n",
    "\n",
    "class State(TypedDict):\n",
    "\tquestion: str\n",
    "\tinitial_answer: str\n",
    "\tcode: str\n",
    "\tresult: str\n",
    "\tstep_list: List[Step]\n",
    "\tprediction: str\n",
    "\tanswer: str\n"
   ],
   "id": "22e59c9c9d5c2383",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:38:31.012430Z",
     "start_time": "2024-12-13T06:38:29.802034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "\n",
    "plan_prompt:ChatPromptTemplate = hub.pull(\"arietem/math_plan\")\n",
    "\n",
    "plan = plan_prompt | ChatOpenAI(\n",
    "    model=model, temperature=0, top_p=0.95, n=1, base_url=\"https://api.chsdw.top/v1\"\n",
    ")\n",
    "\n",
    "\n",
    "async def plan_step(state: State):\n",
    "\tassert state[\"question\"] is not None\n",
    "\tassert state[\"initial_answer\"] is not None\n",
    "\ttry:\n",
    "\t\tplan_response = await plan.ainvoke({\"question\": state[\"question\"], \"initial_answer\": state[\"initial_answer\"]})\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"plan_step error\", e)\n",
    "\t\treturn {\"prediction\": \"None\"}\n",
    "\n",
    "\tstep_list = re.findall(r'<step>(.*?)</step>', plan_response.content, re.DOTALL)\n",
    "\tstep_list = [{\"step\": step, \"error_prone_points\": [], \"evidence\": None, \"result\": None} for step in step_list]\n",
    "\treturn {\n",
    "\t\t\"step_list\": step_list,\n",
    "\t}"
   ],
   "id": "1259b1d2c3beaf9d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:38:32.159712Z",
     "start_time": "2024-12-13T06:38:31.018977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "error_prone_identification_prompt:ChatPromptTemplate = hub.pull(\"arietem/math_error_prone_identification\")\n",
    "error_prone_identification = error_prone_identification_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o-mini-2024-07-18\", temperature=0, top_p=0.95, n=1, base_url=\"https://api.chsdw.top/v1\"\n",
    ")\n",
    "\n",
    "async def error_prone_identification_step(state: State):\n",
    "\tassert state[\"step_list\"] is not None\n",
    "\tstep_list = \"\\n\".join([f\"step: {step[\"step\"]}\" for step in state[\"step_list\"]])\n",
    "\ttry:\n",
    "\t\terror_prone_identification_response = await error_prone_identification.ainvoke({\"question\": state[\"question\"], \"initial_answer\": state[\"initial_answer\"], \"step_list\": step_list})\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"error_prone_identification error\", e)\n",
    "\t\treturn {\"cot_message\": []}\n",
    "\terror_prone_points_list = error_prone_identification_response.content.split(\"step:\")[1:]\n",
    "\terror_prone_points_list\t= [re.findall(r'point: (.*?)\\n', error_prone_point, re.DOTALL) for error_prone_point in error_prone_points_list]\n",
    "\tstep_list = [\n",
    "\t\t{**step, \"error_prone_points\": error_prone_points}  # 新字典，包含更新\n",
    "\t\tfor step, error_prone_points in zip(state[\"step_list\"], error_prone_points_list)\n",
    "\t]\n",
    "\treturn {\n",
    "\t\t\"step_list\": step_list\n",
    "\t}"
   ],
   "id": "43ea7e4604a634a0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:38:33.238714Z",
     "start_time": "2024-12-13T06:38:32.166803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if dataset_name in [\"gsmhard\", \"gsm8k\"]:\n",
    "\tcode_generation_prompt = hub.pull(\"arietem/pot_generation\")\n",
    "elif dataset_name in [\"tabmwp\"]:\n",
    "\tcode_generation_prompt = hub.pull(\"arietem/tabmwp_pot_generation\")\n",
    "else:\n",
    "\tcode_generation_prompt = hub.pull(\"arietem/math_code_generation\")\n",
    "\n",
    "code_generation = code_generation_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o-mini-2024-07-18\", temperature=0, top_p=0.95, n=1, base_url=\"https://api.chsdw.top/v1\"\n",
    ")\n",
    "\n",
    "async def code_generation_step(state: State):\n",
    "\tassert state[\"step_list\"] is not None\n",
    "\tguidance = [\n",
    "    f\"step: {step['step']}\\n\" + \"\\n\".join([f\"point: {point}\" for point in step[\"error_prone_points\"]])\n",
    "    for step in state[\"step_list\"]\n",
    "\t]\n",
    "\ttry:\n",
    "\t\tresponse = await code_generation.ainvoke({\"question\": state[\"question\"], \"guidance\": \"\\n\".join(guidance) + \"\\n Your response should follow previous pattern, which is a code block.\"})\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"code generation step error\", e)\n",
    "\t\treturn {\"code\": \"None\", \"result\": \"None\"}\n",
    "\tresult = await python_interpreter.arun(response.content)\n",
    "\treturn {\"code\": response.content, \"result\": result}"
   ],
   "id": "d3c9b5709cc06d32",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:38:33.307561Z",
     "start_time": "2024-12-13T06:38:33.246604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Using the provided evidence, answer the question by extracting only the specific information required. \\\n",
    "Your response should contains two part, the first part is the fusion of the Revising Process and the second part is the final answer. \\\n",
    "In the final answer, do not include any explanations, context, or additional information. Just focus on delivering the exact answer as \\\n",
    "concisely as possible!!! There is no need to answer the question in the form of a complete sentence, just provide the answer in the form \\\n",
    "of a noun, time, entity, single number, yes or no, etc.\n",
    "\n",
    "Each part of your response should be enclosed by a XML tag, following the format:\n",
    "<fusion>The fusion of the Revising Process</fusion>\n",
    "<final_answer>The final answer</final_answer>\n",
    "\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "get_final_answer = final_answer_prompt | ChatOpenAI(\n",
    "\tmodel=model, temperature=0, top_p=0.95, n=1, base_url=\"https://api.chsdw.top/v1\"\n",
    ")\n",
    "\n",
    "async def final_answer_step(state: State):\n",
    "\tquestion = f\"Question: {state[\"question\"]}\"\n",
    "\tguidance = \"\\n\".join([\n",
    "    f\"step: {step['step']}\\n\" + \"\\n\".join([f\"point: {point}\" for point in step[\"error_prone_points\"]])\n",
    "    for step in state[\"step_list\"]\n",
    "\t])\n",
    "\tcode = f\"```python\\n{state[\"code\"]}```\"\n",
    "\tresult = state[\"result\"]\n",
    "\tfusion = \"None\"\n",
    "\tprediction = \"None\"\n",
    "\ttry:\n",
    "\t\tfinal_answer = await get_final_answer.ainvoke({\"messages\": [f\"{question}\\n\\n{guidance}\\n\\n{code}\\n\\n{result}\"]})\n",
    "\t\tfusion = re.findall(r\"<fusion>(.*?)</fusion>\", final_answer.content, re.DOTALL)[-1]\n",
    "\t\tprediction = re.findall(r\"<final_answer>(.*?)</final_answer>\", final_answer.content, re.DOTALL)[-1]\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"final_answer_step\", e)\n",
    "\t\treturn {\"fusion\":fusion, \"prediction\": prediction}\n",
    "\n",
    "\treturn {\"fusion\": fusion, \"prediction\": prediction}"
   ],
   "id": "45c234ff448f706d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:38:33.320801Z",
     "start_time": "2024-12-13T06:38:33.315867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add the plan node\n",
    "workflow.add_node(\"plan\", plan_step)\n",
    "\n",
    "# Add the error identification node\n",
    "workflow.add_node(\"error_identification\", error_prone_identification_step)\n",
    "\n",
    "workflow.add_node(\"code_generation\", code_generation_step)\n",
    "\n",
    "workflow.add_node(\"get_final_answer\", final_answer_step)\n",
    "\n",
    "workflow.add_edge(START, \"plan\")\n",
    "\n",
    "# From plan we go to error identification\n",
    "workflow.add_edge(\"plan\", \"error_identification\")\n",
    "\n",
    "workflow.add_edge(\"error_identification\", \"code_generation\")\n",
    "\n",
    "workflow.add_edge(\"code_generation\", \"get_final_answer\")\n",
    "\n",
    "workflow.add_edge(\"get_final_answer\", END)\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ],
   "id": "74f9f2a342aa382d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T06:35:57.490195Z",
     "start_time": "2024-12-13T06:35:49.078304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async for event in app.astream({**dataset[900]},):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)\n",
    "print(dataset[900])"
   ],
   "id": "46f731dbf93a9ee0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step_list': [{'step': ' Identify the waiting times for April and May from the table. ', 'error_prone_points': [], 'evidence': None, 'result': None}, {'step': ' Calculate the difference in waiting times between April and May to find the rate of change. ', 'error_prone_points': [], 'evidence': None, 'result': None}, {'step': ' Execute the Python code to compute the rate of change. ', 'error_prone_points': [], 'evidence': None, 'result': None}]}\n",
      "{'step_list': [{'step': ' Identify the waiting times for April and May from the table. ', 'error_prone_points': ['There is potential for error if the administrator misreads the table or records the wrong values for April and May. It is important to double-check the values: April is 18 minutes and May is 17 minutes.'], 'evidence': None, 'result': None}, {'step': ' Calculate the difference in waiting times between April and May to find the rate of change. ', 'error_prone_points': ['The description incorrectly states that the rate of change is simply the difference between waiting times. The rate of change should be calculated as the difference divided by the number of months (which is 1 in this case), leading to the misunderstanding of the formula needed.'], 'evidence': None, 'result': None}, {'step': ' Execute the Python code to compute the rate of change. ', 'error_prone_points': [], 'evidence': None, 'result': None}]}\n",
      "code generation step error \"Input to ChatPromptTemplate is missing variables {'context'}.  Expected: ['context', 'question'] Received: ['question', 'guidance']\\nNote: if you intended {context} to be part of the string and not a variable, please escape it with double curly braces like: '{{context}}'.\"\n",
      "{'code': 'None', 'result': 'None'}\n",
      "{'prediction': '-1'}\n",
      "{'context': 'Read the following table regarding \"Average waiting time at the DMV\" and then answer a question.\\n\\nMonth | Waiting time (minutes)\\nMarch | 13\\nApril | 18\\nMay | 17\\nJune | 17\\nJuly | 16', 'question': 'Read the following table regarding \"Average waiting time at the DMV\" and then answer a question.\\n\\nMonth | Waiting time (minutes)\\nMarch | 13\\nApril | 18\\nMay | 17\\nJune | 17\\nJuly | 16\\nAn administrator at the Department of Motor Vehicles (DMV) tracked the average wait time from month to month. According to the table, what was the rate of change between April and May?', 'answer': '-1', 'ques_type': 'free_text', 'choices': None, 'initial_answer': '```python\\n# Python code, return answer \\nwaiting_time_april = 18\\nwaiting_time_may = 17\\n\\n# Calculate the rate of change between April and May\\nrate_of_change = waiting_time_may - waiting_time_april\\nanswer = rate_of_change\\nprint(answer)```'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:04:40.988866Z",
     "start_time": "2024-12-13T07:04:40.968507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.asyncio import tqdm, tqdm_asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "\n",
    "nest_asyncio.apply()\n",
    "results = []\n",
    "async def process(item):\n",
    "\ttry:\n",
    "\t\tstate = await app.ainvoke({**item})\n",
    "\t\treturn {**item, **state}\n",
    "\texcept:\n",
    "\t\treturn {**item, \"prediction\": \"None\"}\n",
    "async def self_improve_inference() -> None:\n",
    "    error_indices = []  # 用于记录包含 \"ERROR\" 的条目索引\n",
    "\n",
    "    # 读取已有结果或初始化文件\n",
    "    if os.path.exists(save_results_path):\n",
    "        with open(save_results_path, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                result = json.loads(line)\n",
    "                results.append(result)\n",
    "                # 检查是否存在 \"prediction: ERROR\"\n",
    "                if not result.get(\"code\"):\n",
    "                    error_indices.append(idx)\n",
    "    else:\n",
    "        folder_path = os.path.dirname(save_results_path)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "    # print(error_indices)\n",
    "    # raise Exception(\"stop\")\n",
    "\n",
    "    # 重新推理错误的数据\n",
    "    if error_indices:\n",
    "        print(f\"Found {len(error_indices)} ERROR entries. Retrying inference...\")\n",
    "        error_data = [dataset[idx] for idx in error_indices]\n",
    "        new_results = await tqdm_asyncio.gather(*(process(item) for item in error_data))\n",
    "        # 更新原始结果\n",
    "        for i, new_result in zip(error_indices, new_results):\n",
    "            results[i] = new_result\n",
    "\n",
    "    for idx in tqdm(range(len(results), dataset.num_rows, batch_size)):\n",
    "\t    batch = dataset.select(range(idx, min(idx+batch_size, dataset.num_rows)))\n",
    "\t    results.extend(await tqdm_asyncio.gather(*(process(item) for item in batch)))\n",
    "\t    with open(save_results_path, 'w') as file:\n",
    "\t\t    for result in results:\n",
    "\t\t\t    file.write(json.dumps(result) + \"\\n\")\n"
   ],
   "id": "fd772cb5268a766c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:09:06.106339Z",
     "start_time": "2024-12-13T07:04:42.350326Z"
    }
   },
   "cell_type": "code",
   "source": "await self_improve_inference()",
   "id": "e0e307798070e6bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 ERROR entries. Retrying inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [04:23<00:00,  6.76s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:09:16.795910Z",
     "start_time": "2024-12-13T07:09:16.755751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(save_results_path, 'w') as file:\n",
    "    for result in results:\n",
    "\t    file.write(json.dumps(result) + \"\\n\")"
   ],
   "id": "ebff0550d9e77a99",
   "outputs": [],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
