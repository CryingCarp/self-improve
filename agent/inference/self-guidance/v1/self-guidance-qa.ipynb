{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:30:39.964155Z",
     "start_time": "2025-01-23T04:30:39.905237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from agent.utils.loader import load_processed_data\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"self-correct\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://127.0.0.1:7890\""
   ],
   "id": "efeb283497cde6c9",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:30:44.364609Z",
     "start_time": "2025-01-23T04:30:41.062852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "meta_info = {\n",
    "\t\"dataset_name\": 'ambig_qa',\n",
    "\t\"mode\": \"self-improve\",\n",
    "\t\"base_mode\": \"cot\",\n",
    "\t\"model\": \"gpt-3.5-turbo\",\n",
    "\t\"num_samples\": -1,\n",
    "\t\"top_p\": 0.95,\n",
    "\t\"temperature\": 0,\n",
    "\t\"seed\": 42,\n",
    "\t\"batch_size\": 100\n",
    "}\n",
    "assert meta_info[\"mode\"] == \"self-improve\"\n",
    "assert meta_info[\"dataset_name\"] in [\"hotpot_qa\", \"trivia_qa\", \"ambig_qa\"], \"Invalid dataset name\"\n",
    "\n",
    "ROOT_DIR = \"D:\\Projects\\self-improve\"\n",
    "processed_data_path = os.path.join(ROOT_DIR, \"data\", \"processed_data\", f\"{meta_info['dataset_name']}.jsonl\")\n",
    "print(\"Loading processed data from:\", processed_data_path)\n",
    "\n",
    "print(\"Setting the save_results_path\")\n",
    "save_results_path = os.path.join(ROOT_DIR, \"output\", \"inference\", meta_info[\"model\"], meta_info[\"dataset_name\"], meta_info[\"mode\"], f\"with_question_before_fusion_{meta_info['base_mode']}_num_samples_{meta_info['num_samples']}_top_p_{meta_info['top_p']}_temperature_{meta_info['temperature']}_seed_{meta_info['seed']}.jsonl\")\n",
    "print(\"Results will be saved to:\", save_results_path)\n",
    "dataset = load_processed_data(dataset_name=meta_info[\"dataset_name\"], file_path=processed_data_path)\n",
    "if meta_info[\"num_samples\"] > 0:\n",
    "\tdataset = dataset.select(range(meta_info[\"num_samples\"]))\n",
    "print(\"Loaded dataset with\", dataset)\n",
    "print(\"Example:\", dataset[0])\n",
    "\n",
    "print(\"Creating ChatOpenAI model\")\n",
    "model = ChatOpenAI(\n",
    "\tmodel_name=meta_info[\"model\"],\n",
    "\ttop_p=meta_info[\"top_p\"],\n",
    "\ttemperature=meta_info[\"temperature\"],\n",
    "\tseed=meta_info[\"seed\"],\n",
    "\topenai_api_base=\"https://api.chsdw.top/v1\"\n",
    ")\n",
    "print(\"Model created: \", model.model_name)"
   ],
   "id": "f79dc6013e0a4dcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from: D:\\Projects\\self-improve\\data\\processed_data\\ambig_qa.jsonl\n",
      "Setting the save_results_path\n",
      "Results will be saved to: D:\\Projects\\self-improve\\output\\inference\\gpt-3.5-turbo\\ambig_qa\\self-improve\\with_question_before_fusion_cot_num_samples_-1_top_p_0.95_temperature_0_seed_42.jsonl\n",
      "Loaded dataset with Dataset({\n",
      "    features: ['context', 'question', 'answer'],\n",
      "    num_rows: 2002\n",
      "})\n",
      "Example: {'context': '', 'question': 'Who plays the doctor in dexter season 1?', 'answer': ['Goldwyn', 'Tony Goldwyn']}\n",
      "Creating ChatOpenAI model\n",
      "Model created:  gpt-3.5-turbo\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:30:48.335318Z",
     "start_time": "2025-01-23T04:30:48.313287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Sequence\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langgraph.managed.is_last_step import RemainingSteps\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "\tquestion: str\n",
    "\tguidance: str\n",
    "\tremaining_steps: RemainingSteps\n",
    "\tmessages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\tinitial_answer: str\n",
    "\tfusion: str\n",
    "\tprediction: str"
   ],
   "id": "32bf276ff4d80ddd",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:13:02.310722Z",
     "start_time": "2025-01-21T02:13:02.303990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "guidance_prompt = ChatPromptTemplate.from_messages(\n",
    "\t[\n",
    "\t\t(\n",
    "\t\t\t\"system\",\n",
    "\t\t\t\"You are a question planner and error prone points identifier. Given a question or problem, your job is to come up with a step by step plan, and you should also identify the most error-prone points for each step, following them closely behind each step. The plan and the error prone points will then be used to guide the selection of subsequent tools and the corresponding tool inputs. The tool results should always be considered as reliable. Do not add any superfluous steps. Make sure that each step has all the information needed - do not skip steps. You should focus on the logic of how to solve the problem, rather than actually solving it.\"\n",
    "\t\t),\n",
    "\t\t(\n",
    "\t\t\t\"user\",\n",
    "\t\t\t\"Question: {question}\"\n",
    "\t\t)\n",
    "\t])\n",
    "guidance_generator = guidance_prompt | model\n",
    "\n",
    "\n",
    "async def guidance_node(state: State) -> State:\n",
    "\tassert state[\"question\"] is not None, \"Question is required\"\n",
    "\tquestion:str = state[\"question\"]\n",
    "\tguidance:AIMessage = await guidance_generator.ainvoke(input={\"question\": question})\n",
    "\tstate[\"guidance\"] = guidance.content\n",
    "\treturn state"
   ],
   "id": "c127897a59b277b7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:13:06.396377Z",
     "start_time": "2025-01-21T02:13:04.737885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_community.utilities.wikidata import WikidataAPIWrapper\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from agent.utils.tools import GoogleSearchTool, GoogleKnowledgeGraphTool, WikidataTool, WikipediaTool, python_interpreter\n",
    "\n",
    "google_search = GoogleSearchTool()\n",
    "google_knowledge_graph = GoogleKnowledgeGraphTool()\n",
    "wikidata = WikidataTool(api_wrapper=WikidataAPIWrapper())\n",
    "wikipedia = WikipediaTool()\n",
    "tools = [google_search, google_knowledge_graph, wikipedia, wikidata, python_interpreter]\n",
    "\n",
    "model_with_tools = model.bind_tools(tools)\n"
   ],
   "id": "72fcaf53f262f679",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:13:13.207617Z",
     "start_time": "2025-01-21T02:13:13.182981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "\n",
    "critique_prompt = ChatPromptTemplate.from_messages([\n",
    "\t(\n",
    "\t\t\"system\",\n",
    "\t\t\"You are a reactive agent. Given a question or problem, your job is to select the appropriate tools to answer the question or solve the problem. You should consider the guidance provided by the question planner and error prone points identifier, and the tool results are reliable. If you find the answer from the tool results, you should provide the answer.\"\n",
    "\t),\n",
    "\t(\n",
    "\t\t\"user\",\n",
    "\t\t\"Question: {question}\"\n",
    "\t\t\"Guidance: {guidance}\"\n",
    "\t)\n",
    "])\n",
    "\n",
    "async def critique_node(state: State):\n",
    "\tassert state[\"question\"] is not None, \"Question is required\"\n",
    "\tassert state[\"guidance\"] is not None, \"Guidance is required\"\n",
    "\tquestion:str = state[\"question\"]\n",
    "\tguidance:str = state[\"guidance\"]\n",
    "\tmessages:list[BaseMessage] = []\n",
    "\tif len(state[\"messages\"]) == 0:\n",
    "\t\tmessages = critique_prompt.invoke(input={\"question\": question, \"guidance\": guidance}).to_messages()\n",
    "\t\tcritique:AIMessage = await model_with_tools.ainvoke(input=messages)\n",
    "\t\tmessages.append(critique)\n",
    "\telse:\n",
    "\t\tcritique:AIMessage = await model_with_tools.ainvoke(input=state[\"messages\"])\n",
    "\t\tmessages.append(critique)\n",
    "\treturn {\"messages\": messages}\n",
    "\n",
    "# Define our tool node\n",
    "tool_node = ToolNode(tools)\n",
    "# Define our tool node\n",
    "\n",
    "\n",
    "fusion_prompt = ChatPromptTemplate.from_messages(\n",
    "\t[\n",
    "\t\t(\n",
    "\t\t\t\"system\",\n",
    "\t\t\t\"You are a fusion agent. Given a question or problem and based on the critique process, your job is to fuse the tool results and then revise your final answer. \"\n",
    "\t\t\t\"Your response should contains two part, the first part is the fusion of the Revising Process and the second part is the final answer. In the fusion part, you should extract the information from the tool result and also indicate how you obtained the information.(which tool, which part of the result) In the final answer, \"\n",
    "\t\t\t\"do not include any explanations, context, or additional information. Just focus on delivering the exact answer as concisely as possible!!! \"\n",
    "\t\t\t\"There is no need to answer the question in the form of a complete sentence, just provide the answer in the form of a noun, time, entity, single number, yes or no, etc.\"\n",
    "\t\t),\n",
    "\t\t(\n",
    "\t\t\t\"placeholder\",\n",
    "\t\t\t\"{messages}\"\n",
    "\t\t),\n",
    "\t\t(\n",
    "\t\t\t\"user\",\n",
    "\t\t\t\"Question: {question}\"\n",
    "\t\t\t\"Now based on the previous information, please fuse the tool results and revise your answer. Use the XML tag <fusion></fusion> to indicate the fusion part and <answer></answer> to indicate the final answer part. Do not provide multiple answers in the final answer to increase your chances of getting the answer right. You need to give the answer you think is the most appropriate.\"\n",
    "\t\t)\n",
    "\t])\n",
    "fusion_generator = fusion_prompt | model\n",
    "\n",
    "async def fusion_node(state: State) -> State:\n",
    "\tassert state[\"question\"] is not None, \"Question is required\"\n",
    "\tassert state[\"guidance\"] is not None, \"Guidance is required\"\n",
    "\tcritique_messages:Sequence[BaseMessage] = state[\"messages\"][1:]\n",
    "\tresponse:AIMessage = await fusion_generator.ainvoke(input={\"messages\": critique_messages, \"question\": state[\"question\"]})\n",
    "\tfusion_matches = re.findall(r\"<fusion>(.*?)</fusion>\", response.content, re.DOTALL)\n",
    "\tanswer_matches = re.findall(r\"<answer>(.*?)</answer>\", response.content, re.DOTALL)\n",
    "\tif fusion_matches:\n",
    "\t\tstate[\"fusion\"] = fusion_matches[0]\n",
    "\telse:\n",
    "\t\tstate[\"fusion\"] = response.content\n",
    "\tif answer_matches:\n",
    "\t\tstate[\"prediction\"] = answer_matches[0]\n",
    "\telse:\n",
    "\t\tstate[\"prediction\"] = \"None\"\n",
    "\n",
    "\treturn state\n",
    "\n",
    "# Define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: State) -> Literal[\"fuse\", \"tools\"]:\n",
    "\tmessages = state[\"messages\"]\n",
    "\tlast_message = messages[-1]\n",
    "\n",
    "\t# If there is no function call, then we finish\n",
    "\tif last_message.tool_calls:\n",
    "\t\treturn \"tools\"\n",
    "\t# Otherwise if there is, we continue\n",
    "\telse:\n",
    "\t\treturn \"fuse\"\n",
    "\n",
    "def tools_router(state: State) -> Literal[\"fuse\",  \"critique\"]:\n",
    "\tif state[\"remaining_steps\"] <= 3:\n",
    "\t\treturn \"fuse\"\n",
    "\telse:\n",
    "\t\treturn \"critique\""
   ],
   "id": "967d09a412c74673",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:13:20.678586Z",
     "start_time": "2025-01-21T02:13:19.567853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"guide\", guidance_node)\n",
    "workflow.add_node(\"critique\", critique_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"fuse\", fusion_node)\n",
    "\n",
    "workflow.set_entry_point(\"guide\")\n",
    "workflow.add_edge(\"guide\", \"critique\")\n",
    "workflow.add_conditional_edges(\"tools\", tools_router)\n",
    "workflow.add_conditional_edges(\"critique\", should_continue)\n",
    "workflow.add_edge(\"fuse\", \"__end__\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ],
   "id": "5bfebe78d095d52a",
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T04:34:55.200740Z",
     "start_time": "2025-01-23T04:34:46.777570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = {**dataset[5], \"messages\": []}\n",
    "async for event in app.astream(inputs):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ],
   "id": "67762fc90edf407a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'When was the first airline meal served during a flight?', 'messages': [], 'guidance': 'Step 1: Research the history of airline meals to determine when the first airline meal was served during a flight.\\n\\nError-prone point: Inaccurate or conflicting historical information regarding the first airline meal may be found during the research process. Be sure to cross-reference sources and verify the accuracy of the information.'}\n",
      "{'messages': [SystemMessage(content='You are a reactive agent. Given a question or problem, your job is to select the appropriate tools to answer the question or solve the problem. You should consider the guidance provided by the question planner and error prone points identifier, and the tool results are reliable. If you find the answer from the tool results, you should provide the answer.', additional_kwargs={}, response_metadata={}, id='7a817e99-c6a8-4dfd-b5c7-7d86698e38ca'), HumanMessage(content='Question: When was the first airline meal served during a flight?Guidance: Step 1: Research the history of airline meals to determine when the first airline meal was served during a flight.\\n\\nError-prone point: Inaccurate or conflicting historical information regarding the first airline meal may be found during the research process. Be sure to cross-reference sources and verify the accuracy of the information.', additional_kwargs={}, response_metadata={}, id='c8e9bf21-9b42-42aa-9156-2144fd8a7f16'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TYHVynMRKXJDUQ1TzIJ4M3Mv', 'function': {'arguments': '{\"query\":\"History of airline meals\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 689, 'total_tokens': 705, 'completion_tokens_details': None}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-27b41607-7830-46f3-b8e7-336073c8ea23-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'History of airline meals'}, 'id': 'call_TYHVynMRKXJDUQ1TzIJ4M3Mv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 689, 'output_tokens': 16, 'total_tokens': 705})]}\n",
      "{'messages': [ToolMessage(content='Page: Airline meal\\nSummary: An airline meal, airline food, or in-flight meal is a meal served to passengers on board a commercial airliner. These meals are prepared by specialist airline catering services and are normally served to passengers using an airline service trolley.\\nThese meals vary widely in quality and quantity across different airline companies and classes of travel. They range from a simple snack or beverage in short-haul economy class to a seven-course gourmet meal in a first class long-haul flight. The types of food offered also vary widely from country to country, and often incorporate elements of local cuisine, sometimes both from the origin and destination countries. When ticket prices were regulated in the American domestic market, food was the primary means by which airlines differentiated themselves.\\n\\n\\n\\nPage: Kosher airline meal\\nSummary: A kosher airline meal is an airline meal that conforms to the standards of kashrut. Many airlines offer the option of kosher meals to passengers if ordered in advance. These not only contain food that is kosher, but also other features to aid observant Jews, such as copies of Tefilat HaDerech (the Traveler\\'s Prayer) and prayers that are recited before and after eating and bread on which the mezonot blessing is recited, thereby enabling observant Jews to consume the bread without washing hands.\\n\"Kosher\" is one of several options for special meals offered to air travelers. Similarly styled meals that are packaged in double wrapping with verifiable kashrut certification are offered in a variety of other settings, such as cruise ships, hospitals, or  catered events. The double wrapping allows for the meals to be heated in a non-kosher oven.\\nOn airlines, kosher meals are the most commonly requested special meal. Kosher meals have become popular even among non-Jewish passengers who perceive kosher foods to be cleaner and healthier. As they cost approximately twice as much as standard meals, airlines may charge more for them.\\n\\nPage: Airline service trolley\\nSummary: An airline service trolley, also known as an airline catering trolley, airline meal trolley, or trolley cart, is a small serving cart supplied by an air carrier for use by flight attendants inside the aircraft for transport of beverages, airline meals, and other items during a flight.', name='wikipedia', id='89ac9b0f-60b9-437b-9a29-3b2f7b717e6c', tool_call_id='call_TYHVynMRKXJDUQ1TzIJ4M3Mv')]}\n",
      "{'messages': [AIMessage(content='The search results provide information on airline meals, but the specific historical information about when the first airline meal was served during a flight is not mentioned. Would you like me to attempt another search or try a different approach to find the answer to your question?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 1177, 'total_tokens': 1228, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f2d1fb2c-77aa-4dde-b5cf-a045fd399d67-0', usage_metadata={'input_tokens': 1177, 'output_tokens': 51, 'total_tokens': 1228})]}\n",
      "{'question': 'When was the first airline meal served during a flight?', 'guidance': 'Step 1: Research the history of airline meals to determine when the first airline meal was served during a flight.\\n\\nError-prone point: Inaccurate or conflicting historical information regarding the first airline meal may be found during the research process. Be sure to cross-reference sources and verify the accuracy of the information.', 'messages': [SystemMessage(content='You are a reactive agent. Given a question or problem, your job is to select the appropriate tools to answer the question or solve the problem. You should consider the guidance provided by the question planner and error prone points identifier, and the tool results are reliable. If you find the answer from the tool results, you should provide the answer.', additional_kwargs={}, response_metadata={}, id='7a817e99-c6a8-4dfd-b5c7-7d86698e38ca'), HumanMessage(content='Question: When was the first airline meal served during a flight?Guidance: Step 1: Research the history of airline meals to determine when the first airline meal was served during a flight.\\n\\nError-prone point: Inaccurate or conflicting historical information regarding the first airline meal may be found during the research process. Be sure to cross-reference sources and verify the accuracy of the information.', additional_kwargs={}, response_metadata={}, id='c8e9bf21-9b42-42aa-9156-2144fd8a7f16'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TYHVynMRKXJDUQ1TzIJ4M3Mv', 'function': {'arguments': '{\"query\":\"History of airline meals\"}', 'name': 'wikipedia'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 689, 'total_tokens': 705, 'completion_tokens_details': None}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-27b41607-7830-46f3-b8e7-336073c8ea23-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'History of airline meals'}, 'id': 'call_TYHVynMRKXJDUQ1TzIJ4M3Mv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 689, 'output_tokens': 16, 'total_tokens': 705}), ToolMessage(content='Page: Airline meal\\nSummary: An airline meal, airline food, or in-flight meal is a meal served to passengers on board a commercial airliner. These meals are prepared by specialist airline catering services and are normally served to passengers using an airline service trolley.\\nThese meals vary widely in quality and quantity across different airline companies and classes of travel. They range from a simple snack or beverage in short-haul economy class to a seven-course gourmet meal in a first class long-haul flight. The types of food offered also vary widely from country to country, and often incorporate elements of local cuisine, sometimes both from the origin and destination countries. When ticket prices were regulated in the American domestic market, food was the primary means by which airlines differentiated themselves.\\n\\n\\n\\nPage: Kosher airline meal\\nSummary: A kosher airline meal is an airline meal that conforms to the standards of kashrut. Many airlines offer the option of kosher meals to passengers if ordered in advance. These not only contain food that is kosher, but also other features to aid observant Jews, such as copies of Tefilat HaDerech (the Traveler\\'s Prayer) and prayers that are recited before and after eating and bread on which the mezonot blessing is recited, thereby enabling observant Jews to consume the bread without washing hands.\\n\"Kosher\" is one of several options for special meals offered to air travelers. Similarly styled meals that are packaged in double wrapping with verifiable kashrut certification are offered in a variety of other settings, such as cruise ships, hospitals, or  catered events. The double wrapping allows for the meals to be heated in a non-kosher oven.\\nOn airlines, kosher meals are the most commonly requested special meal. Kosher meals have become popular even among non-Jewish passengers who perceive kosher foods to be cleaner and healthier. As they cost approximately twice as much as standard meals, airlines may charge more for them.\\n\\nPage: Airline service trolley\\nSummary: An airline service trolley, also known as an airline catering trolley, airline meal trolley, or trolley cart, is a small serving cart supplied by an air carrier for use by flight attendants inside the aircraft for transport of beverages, airline meals, and other items during a flight.', name='wikipedia', id='89ac9b0f-60b9-437b-9a29-3b2f7b717e6c', tool_call_id='call_TYHVynMRKXJDUQ1TzIJ4M3Mv'), AIMessage(content='The search results provide information on airline meals, but the specific historical information about when the first airline meal was served during a flight is not mentioned. Would you like me to attempt another search or try a different approach to find the answer to your question?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 1177, 'total_tokens': 1228, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f2d1fb2c-77aa-4dde-b5cf-a045fd399d67-0', usage_metadata={'input_tokens': 1177, 'output_tokens': 51, 'total_tokens': 1228})], 'fusion': ' The search results did not provide a specific date for the first airline meal served during a flight. The information focused on the general concept and variety of airline meals, their preparation, and service. Since no historical date was mentioned in the sources consulted (Wikipedia pages about airline meals, kosher meals, and service trolleys), I conclude that further research is needed for accurate historical details. ', 'prediction': '1919'}\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T04:22:34.492322Z",
     "start_time": "2025-01-21T04:22:34.460887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import messages_to_dict\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import nest_asyncio\n",
    "\n",
    "# 配置logger\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,  # 设置日志级别\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # 设置日志格式\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"inference.log\"),  # 将日志输出到文件\n",
    "        logging.StreamHandler()  # 也输出到控制台\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"InferenceLogger\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "results = []\n",
    "batch_size = 100\n",
    "\n",
    "async def process(item):\n",
    "    try:\n",
    "        state = await app.ainvoke({**item, \"messages\": []}, config={\"recursion_limit\": 18})\n",
    "        state[\"messages\"] = messages_to_dict(state[\"messages\"])\n",
    "        logger.info(f\"Processed item: {item}\")\n",
    "        return {**item, **state}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing item: {item}. Error: {e}\")\n",
    "        return {**item, \"prediction\": \"None\"}\n",
    "\n",
    "async def self_improve_inference() -> None:\n",
    "    error_indices = []  # 用于记录包含 \"ERROR\" 的条目索引\n",
    "\n",
    "    # 读取已有结果或初始化文件\n",
    "    if os.path.exists(save_results_path):\n",
    "        logger.info(f\"Loading existing results from {save_results_path}\")\n",
    "        with open(save_results_path, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                result = json.loads(line)\n",
    "                results.append(result)\n",
    "                # 检查是否存在 \"prediction: ERROR\"\n",
    "                if \"None\" == result.get(\"prediction\"):\n",
    "                    error_indices.append(idx)\n",
    "    else:\n",
    "        folder_path = os.path.dirname(save_results_path)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        logger.info(f\"Created directory for results: {folder_path}\")\n",
    "\n",
    "    # 重新推理错误的数据\n",
    "    if error_indices:\n",
    "        logger.warning(f\"Found {len(error_indices)} ERROR entries. Retrying inference...\")\n",
    "        error_data = [dataset[idx] for idx in error_indices]\n",
    "        new_results = await tqdm_asyncio.gather(*(process(item) for item in error_data))\n",
    "        # 更新原始结果\n",
    "        for i, new_result in zip(error_indices, new_results):\n",
    "            results[i] = new_result\n",
    "\n",
    "\n",
    "    for idx in range(len(results), dataset.num_rows, batch_size):\n",
    "        batch = dataset.select(range(idx, min(idx + batch_size, dataset.num_rows)))\n",
    "        batch_results = await tqdm_asyncio.gather(*(process(item) for item in batch))\n",
    "        results.extend(batch_results)\n",
    "\n",
    "        logger.info(f\"Processed batch starting at index {idx}\")\n",
    "\n",
    "        # 保存结果\n",
    "        with open(save_results_path, 'w') as file:\n",
    "            for result in results:\n",
    "                file.write(json.dumps(result) + \"\\n\")\n",
    "        logger.info(f\"Saved results to {save_results_path}\")\n"
   ],
   "id": "84f12d6e0ec85f0c",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T04:25:22.848181Z",
     "start_time": "2025-01-21T04:22:36.401157Z"
    }
   },
   "cell_type": "code",
   "source": "await self_improve_inference()",
   "id": "cf8aad5c193ddba2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "  5%|▌         | 1/20 [00:06<02:00,  6.36s/it]\u001B[A\u001B[A\n",
      "\n",
      " 10%|█         | 2/20 [00:09<01:17,  4.29s/it]\u001B[A\u001B[A\n",
      "\n",
      " 15%|█▌        | 3/20 [00:09<00:43,  2.56s/it]\u001B[A\u001B[A\n",
      "\n",
      " 20%|██        | 4/20 [00:09<00:26,  1.64s/it]\u001B[A\u001B[A\n",
      "\n",
      " 25%|██▌       | 5/20 [00:10<00:16,  1.11s/it]\u001B[A\u001B[A\n",
      "\n",
      " 30%|███       | 6/20 [00:11<00:15,  1.11s/it]\u001B[A\u001B[A\n",
      "\n",
      " 35%|███▌      | 7/20 [00:16<00:31,  2.41s/it]\u001B[A\u001B[A\n",
      "\n",
      " 40%|████      | 8/20 [00:21<00:39,  3.29s/it]\u001B[A\u001B[A\n",
      "\n",
      " 45%|████▌     | 9/20 [00:21<00:25,  2.31s/it]\u001B[A\u001B[A\n",
      "\n",
      " 50%|█████     | 10/20 [00:23<00:22,  2.24s/it]\u001B[A\u001B[A--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_35432\\2572487238.py\", line 25, in process\n",
      "    state = await app.ainvoke({**item, \"messages\": []}, config={\"recursion_limit\": 18})\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1989, in ainvoke\n",
      "    async for chunk in self.astream(\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1874, in astream\n",
      "    async for _ in runner.atick(\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 362, in atick\n",
      "    await arun_with_retry(\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 132, in arun_with_retry\n",
      "    return await task.proc.ainvoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 445, in ainvoke\n",
      "    input = await step.ainvoke(input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 236, in ainvoke\n",
      "    ret = await asyncio.create_task(coro, context=context)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_35432\\3470732617.py\", line 26, in critique_node\n",
      "    critique:AIMessage = await model_with_tools.ainvoke(input=state[\"messages\"])\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5366, in ainvoke\n",
      "    return await self.bound.ainvoke(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 307, in ainvoke\n",
      "    llm_result = await self.agenerate_prompt(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 796, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 756, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 924, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 827, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1412, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\openai\\_base_client.py\", line 1829, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\openai\\_base_client.py\", line 1523, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\openai\\_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\openai\\_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\openai\\_base_client.py\", line 1609, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\openai\\_base_client.py\", line 1656, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\openai\\_base_client.py\", line 1624, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 20250121122259552585123RySQvc0d)', 'type': 'tokens', 'param': '', 'code': 'rate_limit_exceeded'}}\n",
      "During task with name 'critique' and id '4ffdb13a-a4ac-bfd2-efbe-32274cb1f78c'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\logging\\__init__.py\", line 1113, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "UnicodeEncodeError: 'gbk' codec can't encode character '\\xf1' in position 99: illegal multibyte sequence\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\tasks.py\", line 360, in __wakeup\n",
      "    self.__step()\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"C:\\Users\\Administrator\\miniconda3\\envs\\self-improve\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_35432\\2572487238.py\", line 30, in process\n",
      "    logger.error(f\"Error processing item: {item}. Error: {e}\")\n",
      "Message: \"Error processing item: {'context': '', 'question': 'Are Bamboo Mañalac and Danny Jones both musicians?', 'answer': ['yes']}. Error: Error code: 429 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 20250121122259552585123RySQvc0d)', 'type': 'tokens', 'param': '', 'code': 'rate_limit_exceeded'}}\"\n",
      "Arguments: ()\n",
      "2025-01-21 12:23:06,635 - ERROR - Error processing item: {'context': '', 'question': 'Are Bamboo Mañalac and Danny Jones both musicians?', 'answer': ['yes']}. Error: Error code: 429 - {'error': {'message': '当前分组上游负载已饱和，请稍后再试 (request id: 20250121122259552585123RySQvc0d)', 'type': 'tokens', 'param': '', 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "\n",
      " 55%|█████▌    | 11/20 [00:30<00:31,  3.52s/it]\u001B[A\u001B[A\n",
      "\n",
      " 60%|██████    | 12/20 [00:50<01:09,  8.63s/it]\u001B[A\u001B[A\n",
      "\n",
      " 65%|██████▌   | 13/20 [00:53<00:49,  7.01s/it]\u001B[A\u001B[A\n",
      "\n",
      " 70%|███████   | 14/20 [00:59<00:39,  6.59s/it]\u001B[A\u001B[A\n",
      "\n",
      " 75%|███████▌  | 15/20 [01:06<00:33,  6.71s/it]\u001B[A\u001B[A\n",
      "\n",
      " 80%|████████  | 16/20 [01:13<00:27,  6.86s/it]\u001B[A\u001B[A\n",
      "\n",
      " 85%|████████▌ | 17/20 [01:51<00:48, 16.24s/it]\u001B[A\u001B[A\n",
      "\n",
      " 90%|█████████ | 18/20 [01:51<00:22, 11.47s/it]\u001B[A\u001B[A\n",
      "\n",
      " 95%|█████████▌| 19/20 [01:53<00:08,  8.47s/it]\u001B[A\u001B[A\n",
      "\n",
      "100%|██████████| 20/20 [02:46<00:00,  8.32s/it]\u001B[A\u001B[A\n"
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
