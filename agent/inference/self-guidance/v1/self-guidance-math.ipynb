{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from agent.utils.loader import load_processed_data\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"self-correct\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\""
   ],
   "id": "4cd3011c37c64e00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "meta_info = {\n",
    "\t\"dataset_name\": 'svamp',\n",
    "\t\"mode\": \"self-improve\",\n",
    "\t\"base_mode\": \"pot\",\n",
    "\t\"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "\t\"num_samples\": -1,\n",
    "\t\"top_p\": 0.95,\n",
    "\t\"temperature\": 0,\n",
    "\t\"seed\": 42,\n",
    "\t\"batch_size\": 100\n",
    "}\n",
    "assert meta_info[\"mode\"] == \"self-improve\"\n",
    "assert meta_info[\"dataset_name\"] in [\"gsm8k\", \"math\", \"gsmhard\", \"tabmwp\", \"svamp\", \"tabmwp1k\"], \"Invalid dataset name\"\n",
    "\n",
    "if meta_info[\"base_mode\"] == \"cot\":\n",
    "\tprocessed_data_path = f\"../../../output/inference/{meta_info['model']}/{meta_info['dataset_name']}/cot/num_samples_1000_top_p_0.95_temperature_0_seed_42.jsonl\"\n",
    "else:\n",
    "\tprocessed_data_path = f\"../../../data/processed_data/{meta_info['dataset_name']}.jsonl\"\n",
    "\n",
    "dataset = load_processed_data(dataset_name=meta_info[\"dataset_name\"], file_path=processed_data_path)\n",
    "if meta_info[\"dataset_name\"] == \"tabmwp\":\n",
    "\tdataset = dataset.select_columns([\"question\", \"answer\", \"ques_type\", \"ans_type\", ])\n",
    "elif meta_info[\"dataset_name\"] in [\"tabmwp1k\", \"svamp\"]:\n",
    "\tdataset = dataset.map(lambda x: {\"question\": f\"{x['context']}\\n\\n{x['question']}\"}).remove_columns([\"context\"])\n",
    "model = ChatOpenAI(\n",
    "\tmodel_name=meta_info[\"model\"],\n",
    "\ttop_p=meta_info[\"top_p\"],\n",
    "\ttemperature=meta_info[\"temperature\"],\n",
    "\tseed=meta_info[\"seed\"],\n",
    "\topenai_api_base=\"https://api.chsdw.top/v1\"\n",
    ")\n",
    "\n",
    "if meta_info[\"num_samples\"] > 0:\n",
    "\tdataset = dataset.select(range(meta_info[\"num_samples\"]))\n",
    "print(dataset[2])"
   ],
   "id": "432fc32eecaae51f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Sequence\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from langgraph.managed.is_last_step import RemainingSteps\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "\tquestion: str\n",
    "\tguidance: str\n",
    "\tremaining_steps: RemainingSteps\n",
    "\tmessages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\tinitial_answer: str\n",
    "\tfusion: str\n",
    "\tprediction: str"
   ],
   "id": "afa6cb0cf6d56639"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "guidance_prompt = ChatPromptTemplate.from_messages(\n",
    "\t[\n",
    "\t\t(\n",
    "\t\t\t\"system\",\n",
    "\t\t\t\"You are a question planner and error prone points identifier. Given a question or problem, your job is to come up with a step by step plan, and you should also identify the most error-prone points for each step, following them closely behind each step. The plan and the error prone points will then be used to guide the selection of subsequent tools and the corresponding tool inputs. The tool results should always be considered as reliable. Do not add any superfluous steps. Make sure that each step has all the information needed - do not skip steps. You should focus on the logic of how to solve the problem, rather than actually solving it.\"\n",
    "\t\t),\n",
    "\t\t(\n",
    "\t\t\t\"user\",\n",
    "\t\t\t\"Question: {question}\"\n",
    "\t\t)\n",
    "\t])\n",
    "guidance_generator = guidance_prompt | model\n",
    "\n",
    "\n",
    "async def guidance_node(state: State) -> State:\n",
    "\tassert state[\"question\"] is not None, \"Question is required\"\n",
    "\tquestion:str = state[\"question\"]\n",
    "\tguidance:AIMessage = await guidance_generator.ainvoke(input={\"question\": question})\n",
    "\tstate[\"guidance\"] = guidance.content\n",
    "\treturn state"
   ],
   "id": "5d78152259117ee4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_community.utilities.wikidata import WikidataAPIWrapper\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from agent.utils.tools import GoogleSearchTool, GoogleKnowledgeGraphTool, WikidataTool, WikipediaTool, python_interpreter\n",
    "\n",
    "google_search = GoogleSearchTool()\n",
    "google_knowledge_graph = GoogleKnowledgeGraphTool()\n",
    "wikidata = WikidataTool(api_wrapper=WikidataAPIWrapper())\n",
    "wikipedia = WikipediaTool()\n",
    "tools = [google_search, google_knowledge_graph, wikipedia, wikidata, python_interpreter]\n",
    "\n",
    "model_with_tools = model.bind_tools(tools)\n"
   ],
   "id": "fdb027d60d55c8b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from typing import Literal\n",
    "\n",
    "critique_prompt = ChatPromptTemplate.from_messages([\n",
    "\t(\n",
    "\t\t\"system\",\n",
    "\t\t\"You are a reactive agent. Given a question or problem, your job is to select the appropriate tools to answer the question or solve the problem. You should consider the guidance provided by the question planner and error prone points identifier, and the tool results are reliable. If you find the answer from the tool results, you should provide the answer.\"\n",
    "\t),\n",
    "\t(\n",
    "\t\t\"user\",\n",
    "\t\t\"Question: {question}\"\n",
    "\t\t\"Guidance: {guidance}\"\n",
    "\t)\n",
    "])\n",
    "\n",
    "async def critique_node(state: State):\n",
    "\tassert state[\"question\"] is not None, \"Question is required\"\n",
    "\tassert state[\"guidance\"] is not None, \"Guidance is required\"\n",
    "\tquestion:str = state[\"question\"]\n",
    "\tguidance:str = state[\"guidance\"]\n",
    "\tmessages:list[BaseMessage] = []\n",
    "\tif len(state[\"messages\"]) == 0:\n",
    "\t\tmessages = critique_prompt.invoke(input={\"question\": question, \"guidance\": guidance}).to_messages()\n",
    "\t\tcritique:AIMessage = await model_with_tools.ainvoke(input=messages)\n",
    "\t\tmessages.append(critique)\n",
    "\telse:\n",
    "\t\tcritique:AIMessage = await model_with_tools.ainvoke(input=state[\"messages\"])\n",
    "\t\tmessages.append(critique)\n",
    "\treturn {\"messages\": messages}\n",
    "\n",
    "# Define our tool node\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "fusion_prompt = ChatPromptTemplate.from_messages(\n",
    "\t[\n",
    "\t\t(\n",
    "\t\t\t\"system\",\n",
    "\t\t\t\"You are a fusion agent. Given a question or problem and based on the critique process, your job is to fuse the tool results and then revise your final answer. \"\n",
    "\t\t\t\"Your response should contains two part, the first part is the fusion of the Revising Process and the second part is the final answer. In the fusion part, you should extract the information from the tool result and also indicate how you obtained the information.(which tool, which part of the result) In the final answer, \"\n",
    "\t\t\t\"do not include any explanations, context, or additional information. Just focus on delivering the exact answer as concisely as possible!!! \"\n",
    "\t\t\t\"There is no need to answer the question in the form of a complete sentence, just provide the answer in the form of a noun, time, entity, single number, yes or no, etc.\"\n",
    "\t\t),\n",
    "\t\t(\n",
    "\t\t\t\"placeholder\",\n",
    "\t\t\t\"{messages}\"\n",
    "\t\t),\n",
    "\t\t(\n",
    "\t\t\t\"user\",\n",
    "\t\t\t\"Now based on the previous information, please fuse the tool results and revise your answer. Use the XML tag <fusion></fusion> to indicate the fusion part and <answer></answer> to indicate the final answer part.\"\n",
    "\t\t)\n",
    "\t])\n",
    "fusion_generator = fusion_prompt | model\n",
    "\n",
    "async def fusion_node(state: State) -> State:\n",
    "\tassert state[\"question\"] is not None, \"Question is required\"\n",
    "\tassert state[\"guidance\"] is not None, \"Guidance is required\"\n",
    "\tcritique_messages:Sequence[BaseMessage] = state[\"messages\"][1:]\n",
    "\tresponse:AIMessage = await fusion_generator.ainvoke(input={\"messages\": critique_messages})\n",
    "\tfusion_matches = re.findall(r\"<fusion>(.*?)</fusion>\", response.content, re.DOTALL)\n",
    "\tanswer_matches = re.findall(r\"<answer>(.*?)</answer>\", response.content, re.DOTALL)\n",
    "\tif fusion_matches:\n",
    "\t\tstate[\"fusion\"] = fusion_matches[0]\n",
    "\telse:\n",
    "\t\tstate[\"fusion\"] = response.content\n",
    "\tif answer_matches:\n",
    "\t\tstate[\"prediction\"] = answer_matches[0]\n",
    "\telse:\n",
    "\t\tstate[\"prediction\"] = \"None\"\n",
    "\n",
    "\treturn state\n",
    "\n",
    "# Define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: State) -> Literal[\"fuse\", \"tools\"]:\n",
    "\tmessages = state[\"messages\"]\n",
    "\tlast_message = messages[-1]\n",
    "\n",
    "\t# If there is no function call, then we finish\n",
    "\tif last_message.tool_calls:\n",
    "\t\treturn \"tools\"\n",
    "\t# Otherwise if there is, we continue\n",
    "\telse:\n",
    "\t\treturn \"fuse\"\n",
    "\n",
    "def tools_router(state: State) -> Literal[\"fuse\",  \"critique\"]:\n",
    "\tif state[\"remaining_steps\"] <= 3:\n",
    "\t\treturn \"fuse\"\n",
    "\telse:\n",
    "\t\treturn \"critique\""
   ],
   "id": "5d54e73d89094e4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"guide\", guidance_node)\n",
    "workflow.add_node(\"critique\", critique_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"fuse\", fusion_node)\n",
    "\n",
    "workflow.set_entry_point(\"guide\")\n",
    "workflow.add_edge(\"guide\", \"critique\")\n",
    "workflow.add_conditional_edges(\"tools\", tools_router)\n",
    "workflow.add_conditional_edges(\"critique\", should_continue)\n",
    "workflow.add_edge(\"fuse\", \"__end__\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ],
   "id": "721cfdac0693b3df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inputs = {**dataset[3], \"messages\": []}\n",
    "async for event in app.astream(inputs):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ],
   "id": "13034f16687e9bd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset[3]",
   "id": "c8563cdb20e5abad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import nest_asyncio\n",
    "\n",
    "# 配置logger\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,  # 设置日志级别\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # 设置日志格式\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"inference.log\"),  # 将日志输出到文件\n",
    "        logging.StreamHandler()  # 也输出到控制台\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"InferenceLogger\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "results = []\n",
    "batch_size = 100\n",
    "save_results_path = f\"../../../output/inference/{meta_info['model']}/{meta_info['dataset_name']}/{meta_info['mode']}/{meta_info['base_mode']}_num_samples_{meta_info['num_samples']}_top_p_{meta_info['top_p']}_temperature_{meta_info['temperature']}_seed_{meta_info['seed']}.jsonl\"\n",
    "\n",
    "async def process(item):\n",
    "    try:\n",
    "        state = await app.ainvoke({**item, \"messages\": []})\n",
    "        state[\"messages\"] = [message.pretty_repr() for message in state[\"messages\"]]\n",
    "        logger.info(f\"Processed item: {item}\")\n",
    "        return {**item, **state}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing item: {item}. Error: {e}\")\n",
    "        return {**item, \"prediction\": \"None\"}\n",
    "\n",
    "async def self_improve_inference() -> None:\n",
    "    error_indices = []  # 用于记录包含 \"ERROR\" 的条目索引\n",
    "\n",
    "    # 读取已有结果或初始化文件\n",
    "    if os.path.exists(save_results_path):\n",
    "        logger.info(f\"Loading existing results from {save_results_path}\")\n",
    "        with open(save_results_path, 'r') as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                result = json.loads(line)\n",
    "                results.append(result)\n",
    "                # 检查是否存在 \"prediction: ERROR\"\n",
    "                if \"None\" == result.get(\"prediction\"):\n",
    "                    error_indices.append(idx)\n",
    "    else:\n",
    "        folder_path = os.path.dirname(save_results_path)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        logger.info(f\"Created directory for results: {folder_path}\")\n",
    "\n",
    "    # 重新推理错误的数据\n",
    "    if error_indices:\n",
    "        logger.warning(f\"Found {len(error_indices)} ERROR entries. Retrying inference...\")\n",
    "        error_data = [dataset[idx] for idx in error_indices]\n",
    "        new_results = await tqdm_asyncio.gather(*(process(item) for item in error_data))\n",
    "        # 更新原始结果\n",
    "        for i, new_result in zip(error_indices, new_results):\n",
    "            results[i] = new_result\n",
    "\n",
    "\n",
    "    for idx in range(len(results), dataset.num_rows, batch_size):\n",
    "        batch = dataset.select(range(idx, min(idx + batch_size, dataset.num_rows)))\n",
    "        batch_results = await tqdm_asyncio.gather(*(process(item) for item in batch))\n",
    "        results.extend(batch_results)\n",
    "\n",
    "        logger.info(f\"Processed batch starting at index {idx}\")\n",
    "\n",
    "        # 保存结果\n",
    "        with open(save_results_path, 'qa') as file:\n",
    "            for result in results:\n",
    "                file.write(json.dumps(result) + \"\\n\")\n",
    "        logger.info(f\"Saved results to {save_results_path}\")"
   ],
   "id": "ea507dced72aea26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "await self_improve_inference()\n",
    "# 保存结果"
   ],
   "id": "2b80ff65791953cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(save_results_path, 'qa') as file:\n",
    "\tfor result in results:\n",
    "\t\tfile.write(json.dumps(result) + \"\\n\")\n",
    "logger.info(f\"Saved results to {save_results_path}\")"
   ],
   "id": "51f0174406d9b735"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
