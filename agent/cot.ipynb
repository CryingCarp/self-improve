{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"cot\"\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# Load prompt\n",
    "def load_prompt(dataset_name: str) -> ChatPromptTemplate:\n",
    "    dataset_prompts = {\n",
    "        \"gsm8k\": \"gsm8k_9shot\",\n",
    "        \"svamp\": \"svamp_7shot\",\n",
    "        \"tabmwp\": \"tabmwp_4shot\",\n",
    "        \"hotpot_qa\": \"hotpot_qa_6shot\",\n",
    "        \"ambig_qa\": \"ambig_qa_5shot\",\n",
    "        \"trivia_qa\": \"trivia_qa_5shot\",\n",
    "    }\n",
    "    \n",
    "    if dataset_name not in dataset_prompts:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not supported\")\n",
    "    \n",
    "    prompt = hub.pull(dataset_prompts[dataset_name])\n",
    "    return prompt\n",
    "    \n",
    "\n",
    "\n",
    "# 并行处理\n",
    "import asyncio\n",
    "\n",
    "async def cot(item, prompt: ChatPromptTemplate, llm: ChatOpenAI, dataset_name:str=\"hotpot_qa\") -> str:\n",
    "    if dataset_name in [\"gsm8k\", \"hotpot_qa\", \"ambig_qa\", \"trivia_qa\"]:\n",
    "        prompt_value: PromptValue = prompt.invoke({\"question\": item[\"question\"]})\n",
    "    elif dataset_name == \"svamp\":\n",
    "        prompt_value = prompt.invoke({\"Body\": item[\"Body\"], \"Question\": item[\"Question\"]})\n",
    "    elif dataset_name == \"tabmwp\":\n",
    "        prompt_value = prompt.invoke({\"question\": item[\"question\"], \"table\": item[\"table\"], \"table_title\": item[\"table_title\"]})\n",
    "    try:\n",
    "        result: BaseMessage = await llm.ainvoke(prompt_value)\n",
    "        return result.content\n",
    "    except Exception as e:\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract question and answer from ambig_qa\n",
    "import random\n",
    "def extract_question_answer(item):\n",
    "    if item['annotations']['type'][0] == \"singleAnswer\":\n",
    "        # single answer\n",
    "        answers = item['nq_answer']\n",
    "        for ans in item['annotations']['answer']:\n",
    "            answers.extend(ans)\n",
    "        item['answer'] = list(set(answers))\n",
    "    else:\n",
    "        # random choose a question with multiple answers\n",
    "        qa_pairs = item['annotations']['qaPairs'][0]\n",
    "        rand_i = random.randint(0, len(qa_pairs['question'])-1)\n",
    "        item['question'] = qa_pairs['question'][rand_i]\n",
    "        item['answer'] = qa_pairs['answer'][rand_i]\n",
    "    \n",
    "    return {\"question\": item[\"question\"], \"answer\": item[\"answer\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gsm8k svamp tabmwp hotpot_qa ambig_qa trivia_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'ambig_qa_5shot', 'lc_hub_commit_hash': 'd229fd2b0f6d226826ed84152137089274dbe51f34abe1dbdeccb180f057bc44'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Q: What airport is closest to Palm Springs?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='A: The nearest airport to Palm Springs is Indio/Palm Springs (PSP) Airport which is 2.1 miles away. FINAL ANSWER: Palm Springs International Airport')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Q: What degree did Martin Luther King get?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='A: Martin Luther King earned his Bachelor of Divinity degree from Crozer Theological Seminary, followed by a doctorate in Systematic Theology from Boston University. FINAL ANSWER: Bachelor of Divinity')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Q: What countries does the Niger river flow through?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='A: The Niger river runs in a crescent through Libya, Mali, Niger, on the border with Benin and then through Nigeria. FINAL ANSWER: Libya')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Q: What type of currency is used in Puerto Rico?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='A: Puerto Rico is a territory of the United States and uses the U.S. dollar. FINAL ANSWER: United States dollar')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Q: Who played kitt in knight rider?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='A: kitt was voice most often by William Daniels. FINAL ANSWER: William Daniels')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Q: {question}'))]\n",
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1000\n",
      "}) {'question': 'Who plays the doctor in dexter season 1?', 'answer': ['Tony Goldwyn', 'Goldwyn']}\n"
     ]
    }
   ],
   "source": [
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "\n",
    "dataset_name = \"ambig_qa\"\n",
    "mode = \"cot\"\n",
    "num_test_sample = 1000\n",
    "temperature = 0\n",
    "top_p = 1\n",
    "batch_size = 100\n",
    "prompt: ChatPromptTemplate = load_prompt(dataset_name)\n",
    "llm = ChatOpenAI(temperature=0, base_url=\"https://api.chsdw.top/v1\", top_p=1, model=\"gpt-4o-mini\")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset: DatasetDict | Dataset | IterableDatasetDict | IterableDataset = load_dataset(\"json\", data_files=f\"../data/{dataset_name}.jsonl\", split=\"train\")\n",
    "if num_test_sample > 0:\n",
    "    dataset = dataset.select(range(num_test_sample))\n",
    "\n",
    "if dataset_name == \"ambig_qa\":\n",
    "    dataset = dataset.map(extract_question_answer, remove_columns=['id', 'annotations', 'viewed_doc_titles', 'used_queries', 'nq_answer', 'nq_doc_title'])\n",
    "print(dataset, dataset[0])\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: In Season 1 of \"Dexter,\" the character Dr. Rudy Cooper, who later becomes known as the Ice Truck Killer, is played by Christian Camargo. FINAL ANSWER: Christian Camargo.\n"
     ]
    }
   ],
   "source": [
    "result: str = await cot(dataset[0], prompt, llm, dataset_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.93it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.79it/s]\n",
      "100%|██████████| 100/100 [00:05<00:00, 17.79it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.85it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.58it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.16it/s]\n",
      "100%|██████████| 100/100 [00:05<00:00, 16.95it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.40it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 16.22it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "for i in range(0, 1000, batch_size):\n",
    "    batch = dataset.select(range(i, min(i+batch_size, len(dataset))))\n",
    "    results.extend(await tqdm_asyncio.gather(*(cot(item, prompt, llm, dataset_name) for item in batch)))\n",
    "    with open(f\"/Users/ariete/Projects/self-improve/output/inference/{dataset_name}/{mode}_{1000}_temperature_{temperature}_top-p_{top_p}.jsonl\", \"w\") as f:\n",
    "        for idx, result in enumerate(results):\n",
    "            # if dataset_name == \"ambig_qa\":\n",
    "            #     if dataset[idx]['annotations']['type'][0] == \"singleAnswer\":\n",
    "            #         # single answer\n",
    "            #         answers = dataset[idx]['nq_answer']\n",
    "            #         for ans in dataset[idx]['annotations']['answer']:\n",
    "            #             answers.extend(ans)\n",
    "            #         answers = list(set(answers))\n",
    "            #     else:\n",
    "            #         # random choose a question with multiple answers\n",
    "            #         qa_pairs = dataset[idx]['annotations']['qaPairs'][0]\n",
    "            #         rand_i = random.randint(0, len(qa_pairs['question'])-1)\n",
    "            #         dataset[idx]['question'] = qa_pairs['question'][rand_i]\n",
    "            #         dataset[idx]['answer'] = qa_pairs['answer'][rand_i]\n",
    "            f.write(json.dumps({\"question\":dataset[idx][\"question\"], \"answer\": dataset[idx][\"answer\"], \"prediction\": result}) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfimprove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
